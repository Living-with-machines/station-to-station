{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "Several external resources have been employed in our experiments. This notebook automates the process of downloading them and storing them in the appropriate locations.\n",
    "\n",
    "Run All cells to complete the downloads. **Beware that this will download >70GB to your local machine. You will need a reliable and stable internet connection for this.**\n",
    "\n",
    "Manual instructions for obtaining the resources are included too.\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "- [Download resources from Zenodo](#download-resources-from-zenodo)\n",
    "- [Obtain remaining resources](#obtain-remaining-resources)\n",
    "- [Resources file structure](#resources-file-structure)\n",
    "- [Additional information on shared resources](#additional-information-on-shared-resources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from os import path\n",
    "\n",
    "# A handy wrapper function\n",
    "def download_and_unzip(source_url, dest_dir):\n",
    "    response = requests.get(source_url)\n",
    "    zipped_data = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "    zipped_data.extractall(dest_dir)\n",
    "\n",
    "root_local_dir = path.dirname(path.realpath(\"__file__\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zento\n",
    "\n",
    "Many of the resources we use for our experiments can be downloaded from [Zento](https://zenodo.org/record/5520883). \n",
    "\n",
    "#### Automated download:\n",
    "\n",
    "The cell below automates this download. Some of the directories will be empty at this stages. They will be populated by some of the later cells.\n",
    "\n",
    "#### Manual download:\n",
    "\n",
    "Download the compressed file `resources.zip` and unzip it. Our code assumes the following directory structure:\n",
    "\n",
    "```\n",
    "station-to-station/\n",
    "├── ...\n",
    "├── resources/\n",
    "│   ├── deezymatch/\n",
    "│   ├── geonames/\n",
    "│   ├── geoshapefiles/\n",
    "│   ├── quicks/\n",
    "│   ├── ranklib/\n",
    "│   ├── wikidata/\n",
    "│   ├── wikigaz/\n",
    "│   └── wikipedia/\n",
    "└── ...\n",
    "```\n",
    "\n",
    "Some of the directories will be empty, because we cannot share all the resources we used in our experiments. Please follow the instructions below to obtain the remaining files and store them in the right location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zento_url = \"https://zenodo.org/record/5520883/files/resources.zip?download=1\"\n",
    "zento_dest_dir = root_local_dir\n",
    "\n",
    "# Get resources from Zento\n",
    "download_and_unzip(zento_url, zento_dest_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geonames\n",
    "\n",
    "#### Automated download:\n",
    "The cell below downloads both required Geonames files.\n",
    "\n",
    "#### Manual download:\n",
    "Download the [GB table](http://download.geonames.org/export/dump/GB.zip), and store the unzipped file (`GB.txt`) under `resources/geonames/`.\n",
    "> For reference, we have used the `2021-04-26 09:01` version in our experiments.\n",
    "\n",
    "Download the [alternateNameV2 table](http://download.geonames.org/export/dump/alternateNamesV2.zip), and store the unzipped files (`alternateNamesV2.txt` and `iso-languagecodes.txt`) under `resources/geonames/`.\n",
    "> For reference, we have used the `2021-04-26 09:11` version in our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get two Geonames data files\n",
    "geonames_url = \"http://download.geonames.org/export/dump/GB.zip\"\n",
    "alt_geonames_url = \"http://download.geonames.org/export/dump/alternateNamesV2.zip\"\n",
    "geonames_dest_dir = path.join(root_local_dir, \"resources/geonames/\")\n",
    "\n",
    "download_and_unzip(geonames_url, geonames_dest_dir)\n",
    "download_and_unzip(alt_geonames_url, geonames_dest_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boundary-Line™ Data\n",
    "\n",
    "#### Automated download:\n",
    "The cell below downloads the Ordnance Survey Boundary-Line™ data.\n",
    "\n",
    "#### Manual download:\n",
    "\n",
    "Download the Boundary-Line™ ESRI Shapefile from https://osdatahub.os.uk/downloads/open/BoundaryLine (see [licence](http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/)). Unzip it and copy the following files under the `geoshapefiles/` folder:\n",
    "* `Data/Supplementary_Country/country_region.dbf`\n",
    "* `Data/Supplementary_Country/country_region.prj`\n",
    "* `Data/Supplementary_Country/country_region.shp`\n",
    "* `Data/Supplementary_Country/country_region.shx`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Boundary-Line™ Data\n",
    "boundaryline_url = \"https://api.os.uk/downloads/v1/products/BoundaryLine/downloads?area=GB&format=ESRI%C2%AE+Shapefile&redirect\"\n",
    "boundaryline_dest_dir = path.join(root_local_dir, \"resources/geoshapefiles/\")\n",
    "\n",
    "download_and_unzip(boundaryline_url, boundaryline_dest_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranklib\n",
    "\n",
    "#### Automated download:\n",
    "The cell below downloads the Ranklib software library.\n",
    "\n",
    "#### Manual download:\n",
    "\n",
    "Download the Ranklib `.jar` file from the Lemur project [RankLib page](https://sourceforge.net/p/lemur/wiki/RankLib/) and store it in `ranklib/`. In our experiments, we have used version 2.13, available [here](https://sourceforge.net/projects/lemur/files/lemur/RankLib-2.13/). If this is not available anymore, we would suggest that you get the most recent binary [from here](https://sourceforge.net/projects/lemur/files/lemur/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranklib_url = \"https://sourceforge.net/projects/lemur/files/lemur/RankLib-2.13/RankLib-2.13.jar/download\"\n",
    "rank_lib_fpath = path.join(root_local_dir, \"resources/ranklib/RankLib-2.13.jar\")\n",
    "\n",
    "# Download without needing to unzip\n",
    "response = requests.get(ranklib_url)\n",
    "with open(rank_lib_fpath, mode='wb') as jar_file:\n",
    "    jar_file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikidata\n",
    "\n",
    "The wikidata is huge (~70 Gb). Therefore it needs to be downloaded in chunks and it size needs to be verified to ensure that it has all downloaded successfully.\n",
    "\n",
    "\n",
    "#### Automated download:\n",
    "The cell below downloads the wikidata file and verify its size.\n",
    "\n",
    "#### Manual download:\n",
    "\n",
    "Download a full Wikidata dump from [here](https://dumps.wikimedia.org/wikidatawiki/entities/latest-all.json.bz2) and store the `latest-all.json.bz2` file in `wikidata/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore it needs to be downloaded in chunks and it size needs to be\n",
    "# verified to ensure that it has all downloaded successfully.\n",
    "import xml.etree.ElementTree as ET\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def get_details_from_nginx_filelist(filelist_url, filename):\n",
    "    \"\"\"\n",
    "    This function expects the url for a `nginx` sytled directory listing, and a filename appearing on it.\n",
    "    It parses the directory listing to obtain metadata about the named file.\n",
    "    It returns a tuple with the `filename`, the `modified date` (as str) and the filesize (as int).\n",
    "    \"\"\"\n",
    "    search_fname = re.escape(filename)\n",
    "    pattern = (r\"^(?P<filename>\" +\n",
    "                search_fname +\n",
    "                r\")\\s+(?P<datetime>\\d\\d-[a-zA-Z]{3}-20\\d\\d \\d\\d:\\d\\d)\\s+(?P<filesize>\\d+)\"\n",
    "    )\n",
    "\n",
    "    response = requests.get(filelist_url)\n",
    "\n",
    "    # As a minimum, check that it is an nginx site\n",
    "    if not re.search(\"nginx\", response.headers[\"server\"]):\n",
    "        raise ValueError(f\"Not an Nginx webserver: {filelist_url}\")\n",
    "\n",
    "    file_list_txt = BeautifulSoup(response.text).get_text()\n",
    "    match = re.search(pattern, file_list_txt, re.MULTILINE)\n",
    "\n",
    "    if match:\n",
    "        return match.group(\"filename\"), match.group(\"datetime\"), int(match.group(\"filesize\"))\n",
    "\n",
    "    return None\n",
    "    # filesize\n",
    "    print(file_list_txt)\n",
    "\n",
    "# Must have trailing slash\n",
    "wikidata_filelist_url = \"https://dumps.wikimedia.org/wikidatawiki/entities/\"\n",
    "wikidata_fname = \"latest-all.json.bz2\"\n",
    "\n",
    "wikidata_url = urllib.parse.urljoin(wikidata_filelist_url, wikidata_fname)\n",
    "wikidata_fpath = path.join(root_local_dir, \"resources/wikidata\", wikidata_fname)\n",
    "\n",
    "_, _, target_size = get_details_from_nginx_filelist(wikidata_filelist_url, wikidata_fname)\n",
    "print(target_size)\n",
    "print(path.getsize(wikidata_fpath))\n",
    "\n",
    "\n",
    "# Only download if the correct sized file does not exist locally\n",
    "if (not path.exists(wikidata_fpath)) or path.getsize(wikidata_fpath) != target_size:\n",
    "    print(\"Up to date Wikidata not downloaded locally. Downloading now\")\n",
    "\n",
    "    chunk_size = 1024*1024\n",
    "    i = 0\n",
    "\n",
    "    response = requests.get(wikidata_url, stream=True)\n",
    "    print(response.status_code)\n",
    "    print(response.ok)\n",
    "    with open(wikidata_fpath, mode=\"wb\") as fb:\n",
    "        for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "            print(f\"downloaded {i*chunk_size} bytes so far\")\n",
    "            fb.write(chunk)\n",
    "            i = i+1\n",
    "\n",
    "\n",
    "if (not path.exists(wikidata_fpath)) or path.getsize(wikidata_fpath) != target_size:\n",
    "    raise UserWarning(\"warning - wikidata not downloaded correctly\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- End --"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
