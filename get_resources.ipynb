{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from os import path\n",
    "\n",
    "temp_dir = \"temp\"\n",
    "\n",
    "resources_url = \"https://zenodo.org/record/5520883/files/resources.zip?download=1\"\n",
    "resources_dest_dir = path.dirname(path.realpath(\"__file__\"))\n",
    "\n",
    "# # Get resources from Zento\n",
    "response = requests.get(resources_url)\n",
    "zipped_data = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "zipped_data.extractall(resources_dest_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get Geonames data\n",
    "geonames_url = \"http://download.geonames.org/export/dump/GB.zip\"\n",
    "geonames_dest_dir = path.join(resources_dest_dir, \"resources/geonames/\")\n",
    "\n",
    "response = requests.get(geonames_url)\n",
    "zipped_data = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "zipped_data.extractall(geonames_dest_dir)\n",
    "\n",
    "alt_geonames_url = \"http://download.geonames.org/export/dump/alternateNamesV2.zip\"\n",
    "response = requests.get(alt_geonames_url)\n",
    "zipped_data = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "zipped_data.extractall(geonames_dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get Boundary-Lineâ„¢ Data\n",
    "boundaryline_url = \"https://osdatahub.os.uk/downloads/open/BoundaryLine\"\n",
    "boundaryline_url = \"https://api.os.uk/downloads/v1/products/BoundaryLine/downloads?area=GB&format=ESRI%C2%AE+Shapefile&redirect\"\n",
    "boundaryline_dest_dir = path.join(resources_dest_dir, \"resources/geoshapefiles/\")\n",
    "\n",
    "\n",
    "response = requests.get(boundaryline_url)\n",
    "zipped_data = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "zipped_data.extractall(boundaryline_dest_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranklib_url = \"https://sourceforge.net/projects/lemur/files/lemur/RankLib-2.13/RankLib-2.13.jar/download\"\n",
    "rank_lib_fpath = path.join(resources_dest_dir, \"resources/ranklib/RankLib-2.13.jar\")\n",
    "\n",
    "response = requests.get(ranklib_url)\n",
    "with open(rank_lib_fpath, mode='wb') as jar_file:\n",
    "    jar_file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikidata\n",
    "wikidata_url = \"https://dumps.wikimedia.org/wikidatawiki/entities/latest-all.json.bz2\"\n",
    "wikidata_fpath = path.join(resources_dest_dir, \"resources/wikidata/latest-all.json.bz2\")\n",
    "\n",
    "chunk_size = 1024*1024\n",
    "i = 0\n",
    "\n",
    "#  session = requests.Session()\n",
    "response = requests.get(wikidata_url, stream=True)\n",
    "print(response.status_code)\n",
    "print(response.ok)\n",
    "with open(wikidata_fpath, mode=\"wb\") as fb:\n",
    "    for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "        print(f\"{i*chunk_size}\")\n",
    "        fb.write(chunk)\n",
    "        i = i+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73207061006\n",
      "17350737578\n",
      "warning - wikidata not downloaded correctly\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def get_details_from_nginx_filelist(filelist_url, filename):\n",
    "    search_fname = re.escape(filename)\n",
    "    pattern = (r\"^(?P<filename>\" +\n",
    "                search_fname +\n",
    "                r\")\\s+(?P<datetime>\\d\\d-[a-zA-Z]{3}-20\\d\\d \\d\\d:\\d\\d)\\s+(?P<filesize>\\d+)\"\n",
    "    )\n",
    "\n",
    "    response = requests.get(filelist_url)\n",
    "\n",
    "    # As a minimum, check that it is an nginx site\n",
    "    if not re.search(\"nginx\", response.headers[\"server\"]):\n",
    "        raise ValueError(f\"Not an Nginx webserver: {filelist_url}\")\n",
    "\n",
    "    file_list_txt = BeautifulSoup(response.text).get_text()\n",
    "    match = re.search(pattern, file_list_txt, re.MULTILINE)\n",
    "\n",
    "    if match:\n",
    "        return match.group(\"filename\"), match.group(\"datetime\"), int(match.group(\"filesize\"))\n",
    "\n",
    "    return None\n",
    "    # filesize\n",
    "    print(file_list_txt)\n",
    "\n",
    "\n",
    "wikidata_filelist_url = \"https://dumps.wikimedia.org/wikidatawiki/entities\"\n",
    "wikidata_fname = \"latest-all.json.bz2\"\n",
    "\n",
    "_, _, target_size = get_details_from_nginx_filelist(wikidata_filelist_url, wikidata_fname)\n",
    "print(target_size)\n",
    "print(path.getsize(wikidata_fpath))\n",
    "\n",
    "if path.getsize(wikidata_fpath) != target_size:\n",
    "    print(\"warning - wikidata not downloaded correctly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
