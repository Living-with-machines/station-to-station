{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import re\n",
    "import json\n",
    "import collections\n",
    "from lxml import etree\n",
    "from xml.etree.ElementTree import XML\n",
    "from random import shuffle\n",
    "import pathlib\n",
    "import itertools\n",
    "import string\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "docxFileName = \"../resources/quicks/quick_section4.docx\"\n",
    "docxZip = zipfile.ZipFile(docxFileName)\n",
    "documentXML = docxZip.read('word/document.xml')\n",
    "et = etree.XML(documentXML)\n",
    "ns = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using xpath to find main stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mainstation_xpath = './w:r[not(preceding-sibling::w:r//w:t)][not(w:rPr/w:sz[@w:val=\\\"16\\\"])][..//w:b[not(@w:val=\\\"0\\\")]]/w:t[1]'\n",
    "mainstation_xpath2 = './w:r[not(preceding-sibling::w:r//w:t)][not(w:rPr/w:sz[@w:val=\\\"16\\\"])][../w:pPr[w:pStyle]/w:rPr/w:b[@w:val=\\\"0\\\"]]/w:t[1]'\n",
    "mainstation_xpath3 = './w:r[not(preceding-sibling::w:r//w:t)][not(w:rPr/w:sz[@w:val=\\\"16\\\"])][../w:pPr[w:pStyle[@w:val=\\\"Heading1\\\"]]/w:rPr]/w:t[1]'\n",
    "mainstation_xpath4 = './w:r[not(preceding-sibling::w:r//w:t)][not(w:rPr/w:sz[@w:val=\\\"16\\\"])][../w:pPr[w:pStyle[@w:val=\\\"Heading2\\\"]]/w:rPr]/w:t[1]'\n",
    "mainstation_xpath5 = './w:r[not(preceding-sibling::w:r//w:t)][not(w:rPr/w:sz[@w:val=\\\"16\\\"])][../w:pPr[w:pStyle[@w:val=\\\"Heading3\\\"]]/w:rPr]/w:t[1]'\n",
    "mainstation_xpath6 = './w:r[not(preceding-sibling::w:r//w:t)][not(w:rPr/w:sz[@w:val=\\\"16\\\"])][../w:pPr[w:pStyle[@w:val=\\\"Heading4\\\"]]/w:rPr]/w:t[1]'\n",
    "first_token_para_xpath = './w:r[//w:t]/w:t[1]'\n",
    "\n",
    "stations = pd.DataFrame(columns=['station','type','description'])\n",
    "        \n",
    "def is_mainst(para, mainstation, initial_letter, counter):\n",
    "    paraxp = para.xpath(first_token_para_xpath, namespaces=ns)\n",
    "    mainxpath = \"\"\n",
    "    if paraxp:\n",
    "        \n",
    "        # If text is capitalized (with exception for stations starting with \"Mc\")L\n",
    "        if paraxp[0].text.isupper() or paraxp[0].text.startswith(\"Mc\") and paraxp[0].text[2:].isupper():\n",
    "            \n",
    "            # See if xpath matches a mainstation xpath:\n",
    "            mainxpath = para.xpath(mainstation_xpath, namespaces=ns)\n",
    "            if not mainxpath:\n",
    "                mainxpath = para.xpath(mainstation_xpath2, namespaces=ns)\n",
    "            if not mainxpath:\n",
    "                mainxpath = para.xpath(mainstation_xpath3, namespaces=ns)\n",
    "            if not mainxpath:\n",
    "                mainxpath = para.xpath(mainstation_xpath4, namespaces=ns)\n",
    "            if not mainxpath:\n",
    "                mainxpath = para.xpath(mainstation_xpath5, namespaces=ns)\n",
    "            if not mainxpath:\n",
    "                mainxpath = para.xpath(mainstation_xpath6, namespaces=ns)\n",
    "                \n",
    "            # Filter out station names of length 1, station names that start with initial\n",
    "            # of previous main station (e.g. \"Y BOOTHAM JUNCTION\") or that start with an\n",
    "            # open square bracket or parenthesis:\n",
    "            if mainxpath and len(mainxpath[0].text.strip()) > 1 and not mainxpath[0].text.startswith(initial_letter + \" \") and not mainxpath[0].text.startswith(\"[\") and not mainxpath[0].text.startswith(\"(\"):\n",
    "                counter += 1\n",
    "                mainstation = mainxpath[0].text\n",
    "                initial_letter = mainstation[0]\n",
    "                return mainstation, initial_letter, counter\n",
    "            else:\n",
    "                return mainstation, initial_letter, counter\n",
    "        else:\n",
    "            return mainstation, initial_letter, counter\n",
    "    else:\n",
    "        return mainstation, initial_letter, counter\n",
    "\n",
    "mainstation = \"\"\n",
    "initial_letter = \"\"\n",
    "lowerstation = \"\"\n",
    "dText = dict()\n",
    "counter = 0\n",
    "for i, para in enumerate(et.xpath('//w:p', namespaces=ns)):\n",
    "    text = para.xpath('./w:r/w:t', namespaces=ns)\n",
    "    description = \" \".join([t.text for t in text])\n",
    "    mainstation, initial_letter, counter = is_mainst(para, mainstation, initial_letter, counter)\n",
    "    print(mainstation)\n",
    "    description = description.lstrip('\\x01').strip()\n",
    "    if description:\n",
    "        if (counter, mainstation) in dText:\n",
    "            dText[(counter, mainstation)].append(description)\n",
    "        else:\n",
    "            description = re.sub('^(' + re.escape(mainstation) + ')', '\\1', description).lstrip('\\x01').strip()\n",
    "            description = re.sub(r\" +\", \" \", description).lstrip('\\x01').strip()\n",
    "            if description:\n",
    "                dText[(counter, mainstation)] = [description]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing main stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dStations = collections.OrderedDict(dText)\n",
    "\n",
    "indices = []\n",
    "stations = []\n",
    "descriptions = []\n",
    "for k in dStations:\n",
    "    indices.append(k[0])\n",
    "    stations.append(k[1])\n",
    "    descriptions.append(dStations[k])\n",
    "\n",
    "stationdf = pd.DataFrame(columns=[\"Index\", \"Station\", \"Description\"])\n",
    "stationdf[\"Index\"] = indices\n",
    "stationdf[\"Station\"] = stations\n",
    "stationdf[\"Description\"] = descriptions\n",
    "stationdf = stationdf.set_index(\"Index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using regex to find substations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Quicks intro:\n",
    "with open('companies.txt') as json_file:\n",
    "    companies = json.load(json_file)\n",
    "\n",
    "# From Quicks intro:\n",
    "stntypes = [\"AOT\", \"CLO\", \"CO\", \"CO N\", \"CO TT\", \"ND\", \"NG\", \"NON-TT\", \"OP\", \"P\", \"REOP\", \"TT\", \"HL\", \"LL\", \"HB\", \"HBA\", \"NG\", \"TT\"]\n",
    "\n",
    "# Most common last tokens:\n",
    "keywords = [\"PLATFORM\", \"PLATFORMS\", \"HALT\", \"INTERNATIONAL\", \"JUNCTION\", \"CAMP\", \"CENTRAL\", 'ROAD', \n",
    "            'JUNCTION', 'BRIDGE', 'STREET', 'PARK', 'LANE', 'HILL', 'COLLIERY', 'TOWN', 'GREEN', \n",
    "            'CENTRAL', 'CROSSING', 'NORTH', 'LEVEL', 'EAST', 'DOCK', 'WEST', 'GATE', 'CROSS', 'HALT', \n",
    "            'SOUTH', 'MILL', 'END', 'SIDING', 'HALL', 'HOUSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decription(mainst, description, substationId):\n",
    "    print(mainst)\n",
    "    \n",
    "    # Original formatting error such as \"DYKEBAR [\", \"Cal]\"\n",
    "    if mainst.endswith(\"[\"):\n",
    "        mainst = mainst[:-1]\n",
    "        description[0] = \"[\" + description[0]\n",
    "    \n",
    "    dSubstations = dict()\n",
    "    \n",
    "    rsubst = r\"[A-Z ?\\-?\\&? ?]+ \"\n",
    "    rsubstInitial = r\"^(\" + mainst[0] + \"[ |\\-]([A-Z ?\\-?\\&? ?]+)+) \"\n",
    "    \n",
    "    substname = \"\"\n",
    "    \n",
    "    for line in description:\n",
    "        match1 = re.match(rsubst, line)\n",
    "        match2 = re.match(rsubstInitial, line)\n",
    "        if match2:\n",
    "            if not match2.group(0).strip() in companies:\n",
    "                substname = match2.group(0).strip()\n",
    "                substationId += 1\n",
    "        elif match1:\n",
    "            if len(match1.group(0).strip()) > 1 and not match1.group(0).strip() in companies:\n",
    "                substname = match1.group(0).strip()\n",
    "                substationId += 1\n",
    "        if substname == \"\":\n",
    "            substname = mainst\n",
    "            substationId += 1\n",
    "        stup = (substationId, substname)\n",
    "        if not stup in dSubstations:\n",
    "            line = re.sub(r\"^\" + substname, \"\", line)\n",
    "            dSubstations[stup] = line.strip()\n",
    "        else:\n",
    "            dSubstations[stup] += \" \" + line.strip()\n",
    "                \n",
    "    return dSubstations, substationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = ['MainId', 'MainStation', 'SubId', 'SubStation', 'Description']\n",
    "lst = []\n",
    "subInd = 0\n",
    "for i, row in stationdf.iterrows():\n",
    "    main_station = row[\"Station\"]\n",
    "    description = row[\"Description\"]\n",
    "    dSubstations, subInd = process_decription(main_station, description, subInd)\n",
    "    for ss in dSubstations:\n",
    "        lst.append([i, main_station, ss[0], ss[1], dSubstations[ss]])\n",
    "subsdf = pd.DataFrame(lst, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming abbreviated substations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subst_rename(main, sub):\n",
    "    sub = sub.replace(\"&\", \" AND \")\n",
    "    sub = re.sub(' +', ' ', sub)\n",
    "    rsub = []\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "    main = main.translate(translator).split()\n",
    "    sub = sub.translate(translator).split()\n",
    "    if sub != main:\n",
    "        \n",
    "        # Sometimes, first token is split by whitespace. Join split tokens:\n",
    "        # e.g. ['F', 'ISHPONDS']\n",
    "        if len(sub) == 2:\n",
    "            if sub[0] + sub[1] == main[0]:\n",
    "                sub = [sub[0] + sub[1]]\n",
    "        # e.g. ['L', 'ITTLE', 'ORMESBY']\n",
    "        if len(sub) > 2:\n",
    "            if sub[0] + sub[1] == main[0]:\n",
    "                sub = [sub[0] + sub[1]] + sub[2:]\n",
    "        \n",
    "        # Sometimes, first token is split by whitespace. Join split tokens:\n",
    "        # e.g. 'CROSS KEYS' and 'CROSSKEYS'\n",
    "        if len(main) == 2:\n",
    "            if main[0] + main[1] == sub[0]:\n",
    "                main = [main[0] + main[1]]\n",
    "        # e.g. \n",
    "        if len(main) > 2:\n",
    "            if main[0] + main[1] == sub[0]:\n",
    "                main = [main[0] + main[1]] + main[2:]\n",
    "                \n",
    "        # CASE 1: Main station is just one token, substation is more than one token:\n",
    "        if len(main) == 1 and len(sub) > 1:\n",
    "            \n",
    "            # e.g. 'ALTON' and 'ALTON PARK'\n",
    "            if main[0] == sub[0]:\n",
    "                rsub += sub\n",
    "            \n",
    "            # e.g. 'BONNYRIGG' and 'BONNYRIGGE DEPOT'\n",
    "            elif sub[0].startswith(main[0]):\n",
    "                rsub += sub\n",
    "            \n",
    "            # e.g. 'BIRMINGHAM' and 'B NEW STREET'\n",
    "            elif main[0][0] == sub[0][0] and len(sub[0]) == 1:\n",
    "                rsub.append(main[0])\n",
    "                rsub += sub[1:]\n",
    "            \n",
    "            # e.g. 'BARGEDDIE' and 'BARGE DDI E'\n",
    "            elif \"\".join(sub) == main[0]:\n",
    "                rsub += main\n",
    "                \n",
    "            # e.g. 'ALLOA' and 'SOUTH ALLOA'\n",
    "            elif any(x == main[0] for x in sub):\n",
    "                rsub += sub\n",
    "            \n",
    "            # e.g. 'AIRDRIE' and 'COMMONHEAD A NORTH'\n",
    "            elif any(len(x) == 1 and x[0] == main[0][0] for x in sub):\n",
    "                for x in sub:\n",
    "                    if len(x) == 1 and x[0] == main[0][0]:\n",
    "                        rsub.append(main[0])\n",
    "                    else:\n",
    "                        rsub.append(x)\n",
    "                        \n",
    "            # e.g. 'CAERNARVON' and 'CARNARVON CASTLE'\n",
    "            elif any(SequenceMatcher(None, x, main[0]).ratio() >= 0.8 for x in sub):\n",
    "                rsub = sub\n",
    "                \n",
    "            # e.g. 'SOUTHPORT' and 'STEAMPORT MUSEUM'\n",
    "            else:\n",
    "                rsub.append(main[0])\n",
    "                rsub += sub\n",
    "                \n",
    "        # CASE 2: Substation has length 1, mainstation length > 1\n",
    "        elif len(sub) == 1 and len(main) > 1:\n",
    "            \n",
    "            # e.g. 'CLYDACH ON TAWE' and 'CLYDACH'\n",
    "            if any(SequenceMatcher(None, x, sub[0]).ratio() >= 0.8 for x in main):\n",
    "                rsub = sub\n",
    "                \n",
    "            # e.g. 'HIGHGATE ROAD' and 'HL'\n",
    "            elif sub[0] in keywords or sub[0] in stntypes:\n",
    "                rsub += main\n",
    "                rsub += sub\n",
    "                \n",
    "            # e.g. 'BLAENAU FFESTINIOG' and 'DINAS'\n",
    "            else:\n",
    "                rsub = sub\n",
    "                \n",
    "        # CASE 3: Substation has length 1, mainstation length 1\n",
    "        elif len(sub) == 1 and len(main) == 1:\n",
    "            \n",
    "            # e.g. 'BELMONT' and 'JUNCTION'\n",
    "            if sub[0] in keywords or sub[0] in stntypes:\n",
    "                rsub += main\n",
    "                rsub += sub\n",
    "            \n",
    "            # e.g. 'SELHURST' and 'SELHUST'\n",
    "            elif SequenceMatcher(None, sub[0], main[0]).ratio() >= 0.8:\n",
    "                rsub += main\n",
    "            \n",
    "            # e.g. 'WALKER' and 'WALKERGATE'\n",
    "            elif sub[0].startswith(main[0]):\n",
    "                rsub += sub\n",
    "                \n",
    "            # e.g. 'TILBURY' and 'BERTHS'\n",
    "            else:\n",
    "                rsub += main\n",
    "                rsub += sub\n",
    "        \n",
    "        # CASE 4: otherwise\n",
    "        # e.g. 'FINCHLEY ROAD' and 'F R AND FROGNAL'\n",
    "        # e.g. 'B ON S AND QUORN' and 'BARROW ON SOAR AND QUORN'\n",
    "        else:\n",
    "            tsub = []\n",
    "            for x in sub:\n",
    "                if len(x) == 1:\n",
    "                    for m in main:\n",
    "                        if m.startswith(x) and not m in tsub:\n",
    "                            tsub.append(m)\n",
    "                            break\n",
    "                    if not any(m.startswith(x) for m in main):\n",
    "                        tsub.append(x)\n",
    "                else:\n",
    "                    tsub.append(x)\n",
    "                    \n",
    "            rsub = [x for x in tsub]\n",
    "            \n",
    "            # e.g. \"ST A\" or \"S C\" as substation names:\n",
    "            if len(max(rsub, key=len)) <= 2:\n",
    "                rsub = main\n",
    "                \n",
    "        rsub = \" \".join(rsub)\n",
    "        if re.search(r\"\\bLL\\b\", rsub):\n",
    "            rsub = re.sub(r\"\\bLL\\b\", \"LOW LEVEL\", rsub)\n",
    "        if re.search(r\"\\bHL\\b\", rsub):\n",
    "            rsub = re.sub(r\"\\bHL\\b\", \"HIGH LEVEL\", rsub)\n",
    "            \n",
    "        return rsub\n",
    "    else:\n",
    "        return \" \".join(sub)\n",
    "        \n",
    "subsdf['SubStFormatted'] = subsdf.apply(lambda row: subst_rename(row[\"MainStation\"], row[\"SubStation\"]), axis = 1)\n",
    "subsdf = subsdf[[\"MainId\", \"SubId\", \"MainStation\", \"SubStation\", \"SubStFormatted\", \"Description\"]]\n",
    "subsdf.to_pickle('quicks_processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_candranker(gazname, unique_placenames_array):\n",
    "    \"\"\"\n",
    "    This function returns the unique alternate names in a given gazetteer\n",
    "    in the format required by DeezyMatch candidate ranker.\"\"\"\n",
    "    with open(gazname + \".txt\", \"w\") as fw:\n",
    "        for pl in unique_placenames_array:\n",
    "            pl = pl.strip()\n",
    "            if pl:\n",
    "                pl = pl.replace('\"', \"\")\n",
    "                fw.write(pl.strip() + \"\\t0\\tfalse\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_placenames_array = list(set(list(np.array(subsdf[\"MainStation\"]))))\n",
    "format_for_candranker(\"../toponym_matching/toponyms/quicks_mainst_queries\", unique_placenames_array)\n",
    "\n",
    "unique_placenames_array = list(set(list(np.array(subsdf[\"SubStFormatted\"]))))\n",
    "format_for_candranker(\"../toponym_matching/toponyms/quicks_subst_queries\", unique_placenames_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37deezy)",
   "language": "python",
   "name": "py37deezy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
