{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Quicks intro:\n",
    "with open('companies.txt') as json_file:\n",
    "    companies = json.load(json_file)\n",
    "\n",
    "# From Quicks intro:\n",
    "stntypes = [\"AOT\", \"CLO\", \"CO\", \"CO N\", \"CO TT\", \"ND\", \"NG\", \"NON-TT\", \"OP\", \"P\", \"REOP\",\n",
    "            \"TT\", \"HL\", \"LL\", \"HB\", \"HBA\", \"NG\", \"TT\"]\n",
    "\n",
    "# Most common last tokens:\n",
    "keywords = [\"PLATFORM\", \"PLATFORMS\", \"HALT\", \"INTERNATIONAL\", \"JUNCTION\", \"CAMP\", \"CENTRAL\", 'ROAD', \n",
    "            'JUNCTION', 'BRIDGE', 'STREET', 'PARK', 'LANE', 'HILL', 'COLLIERY', 'TOWN', 'GREEN', \n",
    "            'CENTRAL', 'CROSSING', 'NORTH', 'LEVEL', 'EAST', 'DOCK', 'WEST', 'GATE', 'CROSS', 'HALT', \n",
    "            'SOUTH', 'MILL', 'END', 'SIDING', 'HALL', 'HOUSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_altnames(altnames, mainst, subst):\n",
    "    repl = altnames[0]\n",
    "    prev = altnames[1]\n",
    "    altrn = altnames[2]\n",
    "    added = altnames[3]\n",
    "    dropped = altnames[4]\n",
    "\n",
    "    # Add or drop tokens from main or sub station:\n",
    "    # Often the description mentions that a token has been added to\n",
    "    # or dropped from the station name, e.g. \n",
    "    # \"op 1 January 1857; JUNCTION added 1 April 1864\"\n",
    "    # Here we're modifying the alternate names to add or drop these:\n",
    "    modaltnames = []\n",
    "    for x in list(added + dropped):\n",
    "        if not x in mainst:\n",
    "            modaltnames.append(mainst + \" \" + x)\n",
    "        else:\n",
    "            modaltnames.append(re.sub(r\"\\b%s\\b\" % x, \"\", mainst))\n",
    "        if not x in subst:\n",
    "            modaltnames.append(subst + \" \" + x)\n",
    "        else:\n",
    "            modaltnames.append(re.sub(r\"\\b%s\\b\" % x, \"\", subst))\n",
    "    modaltnames = list(set(modaltnames))\n",
    "    altnames = list(altnames)\n",
    "    altnames.append(modaltnames)\n",
    "\n",
    "    # Find full tokens:\n",
    "    # Substations and alternate names are very often abbreviated, e.g.\n",
    "    # \"[...] became Y R & B P 1 November 1870 [...]\",\n",
    "    # where \"Y R & B P\" is short for \"York Road and Battersea Park\",\n",
    "    # where full tokens are found either in the main station, substation\n",
    "    # or in the alternate names. Therefore, in this step we are collecting\n",
    "    # all full tokens and assume that an initial will refer to the first\n",
    "    # mentioned token that starts with this initial.\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    flt_altnames = [mainst]\n",
    "    flt_altnames.append(subst)\n",
    "    flt_altnames += list(set([item for sublist in altnames for item in sublist]))\n",
    "    altname_tokens = []\n",
    "    for altn in flt_altnames:\n",
    "        altn = altn.translate(translator).split()\n",
    "        altname_tokens += [x for x in altn if re.match(r\"\\b[A-Z]{2,}\\b\", x) and x not in altname_tokens and x not in (\"AND\", \"FOR\")]\n",
    "\n",
    "    # Replace initials with token:\n",
    "    proc_altnames = []\n",
    "    for x in list(set(repl + prev + altrn + modaltnames)):\n",
    "        tsub = []\n",
    "        remsub = []\n",
    "        initial_repl = False\n",
    "        for w in x.split():\n",
    "            if w == \"&\":\n",
    "                w = \"AND\"\n",
    "            if len(w) == 1:\n",
    "                for altt in altname_tokens:\n",
    "                    if altt.startswith(w) and not altt in tsub:\n",
    "                        tsub.append(altt)\n",
    "                        remsub.append(w)\n",
    "                        break\n",
    "                if not any(altt.startswith(w) for altt in altname_tokens):\n",
    "                    tsub.append(w)\n",
    "            else:\n",
    "                tsub.append(w)\n",
    "\n",
    "        proc_altnames.append(\" \".join(tsub))\n",
    "\n",
    "    # Only accept an alternate name if it's not a keyword in Quicks and has length of at least three characters:\n",
    "    proc_altnames = [x for x in proc_altnames if len(x) >= 3 and not x in list(companies.keys()) + stntypes + keywords and not x in (mainst, subst)]\n",
    "\n",
    "    return proc_altnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_description(description, mainst, subst):\n",
    "    \n",
    "    alternate_names = []\n",
    "    \n",
    "    # -----------------------------------\n",
    "    # Remove non-printable characters:\n",
    "    description = ''.join([x if x in string.printable else ' ' for x in description])\n",
    "    \n",
    "    # -----------------------------------\n",
    "    # Remove notes (in parentheses), and text in curly and square brackets:\n",
    "    description = re.sub(r'\\([^)]*?\\)', '', description)\n",
    "    description = re.sub(r'\\[[^)]*?\\]', '', description)\n",
    "    description = re.sub(r'\\{[^)]*?\\}', '', description)\n",
    "    \n",
    "    # -----------------------------------\n",
    "    # Remove extra white spaces:\n",
    "    description = re.sub(' +', ' ', description)\n",
    "    \n",
    "    # -----------------------------------\n",
    "    # Capture alternate names...\n",
    "    re_altname = r\"(\\b[A-Z]+(?:[A-Z \\&\\'\\-(St|for|at|on|upon)])*[A-Z])+\\b\"\n",
    "    \n",
    "    # ... in their context:\n",
    "    re_replacedby = r\"\\b(?:[Bb]ecame|[Rr]enamed|[Ll]ater|[Aa]ltered to|[Rr]eplaced by)\\b \" + re_altname\n",
    "    re_previously1 = r\"\\b(?:as|[Ww]as|[Oo]riginally|[Aa]t first|[Ee]arly)\\b:? \" + re_altname\n",
    "    re_previously2 = re_altname + \" (?:until )(?:(?:[0-9]{1,2})? *(?:Jan ?(?:uary)?|Feb ?(?:ruary)?|Mar ?(?:ch)?|Apr ?(?:il)?|May ?|Jun ?(?:e)?|Jul ?(?:y)?|Aug ?(?:ust)?|Sep ?(?:tember)?|Oct ?(?:ober)?|Nov ?(?:ember)?|Dec ?(?:ember)?)? *(?:[12][0-9]{3}))\"\n",
    "    re_previously3 = re_altname + \" (?:until)\"\n",
    "    re_previously3 = r\"\\b(?:[Uu]ntil [0-9]{4}) \" + re_altname\n",
    "    re_alternatively1 = r\"\\b(?:[Rr]eferred to|[Rr]efers to|[Ee]rratically|[Aa]lias|[Bb]rad had|hb had|[Ll]isted under|[Ii]ndiscriminately|[Nn]otice has|where)\\b \" + re_altname\n",
    "    re_alternatively2 = re_altname + \" (?:(?:(?:in )?(?:hb|[Bb]rad|NB|[Mm]urray))|(?:until renamed))\"\n",
    "    re_alternatively3 = re_altname + \" (?:(?:[0-9]{1,2})? *(?:Jan ?(?:uary)?|Feb ?(?:ruary)?|Mar ?(?:ch)?|Apr ?(?:il)?|May ?|Jun ?(?:e)?|Jul ?(?:y)?|Aug ?(?:ust)?|Sep ?(?:tember)?|Oct ?(?:ober)?|Nov ?(?:ember)?|Dec ?(?:ember)?)? *(?:[12][0-9]{3}))\"\n",
    "    re_added1 = r\"\\b(?:[Aa]dded)\\b \" + re_altname\n",
    "    re_added2 = re_altname + \" (?:(?:was )?added)\"\n",
    "    re_dropped1 = r\"\\b(?:[Dd]ropped)\\b \" + re_altname\n",
    "    re_dropped2 = re_altname + \" (?:dropped)\"\n",
    "    re_referenced = r\"\\b(?:[Ss]ee|[Ss]ee under)\\b \" + re_altname\n",
    "    \n",
    "    # Find all occurrences of alternate names in the description:\n",
    "    replacedby = re.findall(re_replacedby, description)\n",
    "    previously = re.findall(re_previously1, description) + re.findall(re_previously2, description) + re.findall(re_previously3, description)\n",
    "    alternatively = re.findall(re_alternatively1, description) + re.findall(re_alternatively2, description) + re.findall(re_alternatively3, description)\n",
    "    added = re.findall(re_added1, description) + re.findall(re_added2, description)\n",
    "    dropped = re.findall(re_dropped1, description) + re.findall(re_dropped2, description)\n",
    "    referenced = re.findall(re_referenced, description)\n",
    "    \n",
    "    alternate_names = (replacedby, previously, alternatively, added, dropped)\n",
    "    proc_altnames = process_altnames(alternate_names, mainst, subst)\n",
    "    \n",
    "    return proc_altnames, referenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"quicks_processed.pkl\")\n",
    "\n",
    "alt_mainId = []\n",
    "alt_substId = []\n",
    "ref_mainId = []\n",
    "ref_substId = []\n",
    "altnames = []\n",
    "referenced = []\n",
    "for i, row in df.iterrows():\n",
    "    t = parse_description(row[\"Description\"], row[\"MainStation\"], row[\"SubStFormatted\"])\n",
    "    for x in t[0]:\n",
    "        alt_mainId.append(row[\"MainId\"])\n",
    "        alt_substId.append(row[\"SubId\"])\n",
    "        altnames.append(x)\n",
    "    for x in t[1]:\n",
    "        ref_mainId.append(row[\"MainId\"])\n",
    "        ref_substId.append(row[\"SubId\"])\n",
    "        referenced.append(x)\n",
    "        \n",
    "# Dataframe of alternate names:\n",
    "df_altnames = pd.DataFrame()\n",
    "df_altnames[\"Altname\"] = altnames\n",
    "df_altnames[\"MainId\"] = alt_mainId\n",
    "df_altnames[\"SubId\"] = alt_substId\n",
    "        \n",
    "# Dataframe of referenced names:\n",
    "df_referenced = pd.DataFrame()\n",
    "df_referenced[\"Referenced\"] = referenced\n",
    "df_referenced[\"MainId\"] = ref_mainId\n",
    "df_referenced[\"SubId\"] = ref_substId\n",
    "\n",
    "df_altnames.to_pickle(\"quicks_altnames_df.pkl\")\n",
    "df_referenced.to_pickle(\"quicks_referenced_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_candranker(gazname, unique_placenames_array):\n",
    "    \"\"\"\n",
    "    This function returns the unique alternate names in a given gazetteer\n",
    "    in the format required by DeezyMatch candidate ranker.\"\"\"\n",
    "    with open(gazname + \".txt\", \"w\") as fw:\n",
    "        for pl in unique_placenames_array:\n",
    "            pl = pl.strip()\n",
    "            if pl:\n",
    "                pl = pl.replace('\"', \"\")\n",
    "                fw.write(pl.strip() + \"\\t0\\tfalse\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_placenames_array = list(set(list(np.array(df_altnames[\"Altname\"]))))\n",
    "format_for_candranker(\"../toponym_matching/toponyms/quicks_altnames\", unique_placenames_array)\n",
    "\n",
    "unique_placenames_array = list(set(list(np.array(df_referenced[\"Referenced\"]))))\n",
    "format_for_candranker(\"../toponym_matching/toponyms/quicks_referenced\", unique_placenames_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37deezy)",
   "language": "python",
   "name": "py37deezy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
