{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "import ast\n",
    "from wikimapper import WikiMapper\n",
    "from pathlib import Path\n",
    "import urllib.parse\n",
    "import json\n",
    "import html\n",
    "from pigeon import annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets:\n",
    "# * bho_dataset: processed BHO\n",
    "# * wikidata_gazetteer: gazetteer built from wikidata\n",
    "# * temp_linking: output of \"align_bho_cands_to_wikidata.ipynb\", having used DeezyMatch\n",
    "\n",
    "bho_dataset = pd.read_csv(\"/home/mcollardanuy/PlaceLinking/bho/bho.csv\", index_col=\"id\", low_memory=False)\n",
    "wikidata_gazetteer = pd.read_csv(\"/home/mcollardanuy/PlaceLinking/wikidata/british_isles.csv\", index_col=0, low_memory=False)\n",
    "temp_linking = pd.read_pickle(\"/home/mcollardanuy/PlaceLinking/toponym_resolution/bho_wikidata/bho_queries_britwikidata_candidates+ids_wikigaz_en_002.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bho_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_gazetteer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_gazetteer.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_linking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dicts(func, *dicts):\n",
    "    default = collections.defaultdict(set)\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():\n",
    "            default[k].add(v)\n",
    "    return {k: func(v) for k, v in default.items()}\n",
    "\n",
    "almost_exact_threshold = 0.5 # Max similarity threshold for almost-exact match\n",
    "likely_match_threshold = 5 # Max similarity threshold for likely match\n",
    "unlikely_match_threshold = 10 # Min similarity threshold for unlikely match\n",
    "\n",
    "description = []\n",
    "wikidata_candidates = []\n",
    "for i, row in bho_dataset.iterrows():\n",
    "    if row[\"redirected\"] == False:\n",
    "        toponyms = ast.literal_eval(row[\"toponyms\"])\n",
    "        wikidata_cands = dict()\n",
    "        for t in toponyms:\n",
    "            temp_wikidata_cands = temp_linking[temp_linking[\"query\"] == t.strip()][\"wikidata_cands\"]\n",
    "            if not temp_wikidata_cands.empty:\n",
    "                temp_wikidata_cands = temp_wikidata_cands.item()\n",
    "                wikidata_cands = combine_dicts(min, wikidata_cands, temp_wikidata_cands)\n",
    "        wikidata_cands = dict(sorted(wikidata_cands.items(), key=lambda item: item[1]))\n",
    "        cand_keys = list(wikidata_cands.keys())\n",
    "        if len(wikidata_cands) >= 1:\n",
    "            # Case 1: multiple almost exact matches\n",
    "            if wikidata_cands[cand_keys[0]] < almost_exact_threshold and wikidata_cands[cand_keys[1]] < almost_exact_threshold:\n",
    "                description.append(\"multiple_exact\")\n",
    "                wikidata_candidates.append(wikidata_cands)\n",
    "            # Case 2: one almost exact match, other competitive potential matches\n",
    "            elif wikidata_cands[cand_keys[0]] < almost_exact_threshold and wikidata_cands[cand_keys[1]] < likely_match_threshold:\n",
    "                description.append(\"unique_exact_with_competition\")\n",
    "                wikidata_candidates.append(wikidata_cands)\n",
    "            # Case 3: one almost exact match, other matches non competitive\n",
    "            elif wikidata_cands[cand_keys[0]] < almost_exact_threshold and wikidata_cands[cand_keys[1]] > likely_match_threshold:\n",
    "                description.append(\"unique_exact_no_competition\")\n",
    "                wikidata_candidates.append(wikidata_cands)\n",
    "            # Case 4: no almost-exact matches, but likely potential matches\n",
    "            elif wikidata_cands[cand_keys[0]] < likely_match_threshold:\n",
    "                description.append(\"no_exact_potential_match\")\n",
    "                wikidata_candidates.append(wikidata_cands)\n",
    "            # Case 5: no almost-exact matches, only less likely potential matches\n",
    "            elif wikidata_cands[cand_keys[0]] > likely_match_threshold and wikidata_cands[cand_keys[0]] <= unlikely_match_threshold:\n",
    "                description.append(\"potential_no_match\")\n",
    "                wikidata_candidates.append(wikidata_cands)\n",
    "            # Case 6: only unlikely potential matches\n",
    "            elif wikidata_cands[cand_keys[0]] > unlikely_match_threshold:\n",
    "                description.append(\"unlikely_match\")\n",
    "                wikidata_candidates.append(wikidata_cands)\n",
    "        # Case 7: no candidates\n",
    "        else:\n",
    "            description.append(\"no_candidates\")\n",
    "            wikidata_candidates.append({})\n",
    "    else:\n",
    "        description.append(\"redirection\")\n",
    "        wikidata_candidates.append({})\n",
    "        \n",
    "bho_dataset[\"linking_scenario\"] = description\n",
    "bho_dataset[\"wikidata_cands\"] = wikidata_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bho_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bho_dataset[bho_dataset[\"linking_scenario\"] == \"unlikely_match\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bho_dataset[\"linking_scenario\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bho_dataset_to_annotate = bho_dataset[bho_dataset[\"linking_scenario\"] != \"redirection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bho_dataset_to_annotate[\"linking_scenario\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = bho_dataset_to_annotate.groupby('linking_scenario').apply(lambda x: x.sample(50, replace=True)).reset_index(drop=True)\n",
    "sampled_df = sampled_df.drop_duplicates(subset=[\"title\", \"toponyms\", \"contextwords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = WikiMapper(\"/resources/wikidata2wikipedia/index_enwiki-20190420.db\")\n",
    "\n",
    "def map_wikidata2wikidump(wikidataId):\n",
    "    path = \"/resources/wikipedia/extractedResources/Aspects/\"\n",
    "    wikititles = mapper.id_to_titles(wikidataId)\n",
    "    wikititles = [urllib.parse.quote(title.replace(\"_\",\" \")) for title in wikititles]\n",
    "    wikidata_text = \"\"\n",
    "    for title in wikititles:\n",
    "        if Path(path + title+\".json\").is_file():\n",
    "            wikidump = path + title+\".json\"\n",
    "            with open(wikidump) as f:\n",
    "                data = json.load(f)\n",
    "                tmp_wkdt_text = \" \".join(data[\"Main\"][\"content\"][1:5])\n",
    "                if len(tmp_wkdt_text) > len(wikidata_text):\n",
    "                    wikidata_text = tmp_wkdt_text\n",
    "    if wikidata_text == \"\":\n",
    "        wikidata_text = \"[No Wikipedia page]\"\n",
    "    return wikidata_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = WikiMapper(\"/resources/wikidata2wikipedia/index_enwiki-20190420.db\")\n",
    "\n",
    "resolutions = []\n",
    "for i, row in sampled_df.iterrows():\n",
    "    bho_title = row[\"title\"]\n",
    "    bho_content = ast.literal_eval(row[\"content\"].strip())\n",
    "    bho_wkcandidates = dict()\n",
    "    wkcds = row[\"wikidata_cands\"]\n",
    "    if wkcds:\n",
    "        min_value = min(wkcds.values())\n",
    "        best_wikidata_matches = [key for key, value in wkcds.items() if value == min_value]\n",
    "        for wkcd in best_wikidata_matches:\n",
    "            wkdf = wikidata_gazetteer[wikidata_gazetteer[\"wikidata_id\"] == wkcd]\n",
    "            wkcd_hc = []\n",
    "            if not wkdf.empty:\n",
    "                # Get location's historical counties:\n",
    "                hcounties = ast.literal_eval(wkdf.iloc[0][\"hcounties\"])\n",
    "                for hc in hcounties:\n",
    "                    hcountydf = wikidata_gazetteer[wikidata_gazetteer[\"wikidata_id\"] == hc]\n",
    "                    if not hcountydf.empty:\n",
    "                        wkcd_hc.append(hcountydf.iloc[0][\"english_label\"])\n",
    "                # Wikidata candidate disambiguators are:\n",
    "                # * English label\n",
    "                # * Wikidata description\n",
    "                # * Wikipedia's content first sentences\n",
    "                # * Historical counties from wikidata\n",
    "                wkcand_disambiguators = (wkdf.iloc[0][\"english_label\"], wkdf.iloc[0][\"description_set\"], map_wikidata2wikidump(wkcd), wkcd_hc)\n",
    "                bho_wkcandidates[wkdf.iloc[0][\"wikidata_id\"]] = wkcand_disambiguators\n",
    "    resolutions.append([bho_title, bho_content, bho_wkcandidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_strings = []\n",
    "for r in resolutions:\n",
    "    longstring = \"=========================\\nBHO ENTRY: \"\n",
    "    longstring += r[0]\n",
    "    longstring += \"\\n\"\n",
    "    longstring += \"=========================\\n\"\n",
    "    longstring += r[1][0].strip()\n",
    "    longstring += \"\\n\"\n",
    "    longstring += \"\\n\"\n",
    "    longstring += \"=========================\\n\"\n",
    "    longstring += \"WIKIDATA CANDIDATES\\n\"\n",
    "    longstring += \"=========================\\n\"\n",
    "    if r[2]:\n",
    "        for cd in r[2]:\n",
    "            if type(r[2][cd][0]) == str:\n",
    "                longstring += \"\\n-------------------------\\n\"\n",
    "                longstring += \"* \" + cd + \" (\" + r[2][cd][0] + \")\"\n",
    "                longstring += \"\\n-------------------------\\n\"\n",
    "                if r[2][cd][3]:\n",
    "                    longstring += \"[Historical county] \" + \", \".join(r[2][cd][3]) + \"\\n\"\n",
    "                description = \"\"\n",
    "                if r[2][cd][2] != \"### No Wikipedia page ###\":\n",
    "                    description = r[2][cd][2].strip()\n",
    "                else:\n",
    "                    description = r[2][cd][1].strip()\n",
    "                longstring += \"[Description] \" + description + \"\\n\"\n",
    "                \n",
    "    longstring += \"=========================\\n\"\n",
    "    resolution_strings.append(longstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = annotate(\n",
    "    [r.splitlines() for r in resolution_strings]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37deezy)",
   "language": "python",
   "name": "py37deezy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
