{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tools import eval_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate candidate selection and ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Split: test\n",
      "Combination: ['stations']\n",
      "\\begin{tabular}{llrrrllllll}\n",
      "\\toprule\n",
      "  Eval &     Approach &    p &  map & retr &     p &   map &  retr &     p &   map &  retr \\\\\n",
      "\\midrule\n",
      "Strict &   exact:stns & 0.66 & 0.68 & 0.71 &    -- &    -- &    -- &    -- &    -- &    -- \\\\\n",
      "Strict & partial:stns & 0.66 & 0.68 & 0.71 &   0.6 &  0.68 &  0.72 &  0.59 &  0.69 &  0.72 \\\\\n",
      "Strict &   deezy:stns & 0.67 & 0.69 & 0.72 &  0.56 &  0.69 &  0.72 &  0.55 &  0.69 &  0.72 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "==========================================\n",
      "\n",
      "==========================================\n",
      "Split: test\n",
      "Combination: ['stations', 'alts']\n",
      "\\begin{tabular}{llrrrllllll}\n",
      "\\toprule\n",
      "  Eval &          Approach &    p &  map & retr &     p &   map &  retr &     p &   map &  retr \\\\\n",
      "\\midrule\n",
      "Strict &   exact:stns+alts & 0.64 & 0.68 & 0.72 &    -- &    -- &    -- &    -- &    -- &    -- \\\\\n",
      "Strict & partial:stns+alts & 0.64 & 0.69 & 0.72 &  0.57 &  0.67 &  0.73 &  0.56 &  0.68 &  0.73 \\\\\n",
      "Strict &   deezy:stns+alts & 0.63 & 0.69 & 0.73 &  0.52 &  0.69 &  0.73 &  0.51 &  0.69 &  0.73 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "==========================================\n",
      "\n",
      "==========================================\n",
      "Split: test\n",
      "Combination: ['stations', 'places', 'alts']\n",
      "\\begin{tabular}{llrrrllllll}\n",
      "\\toprule\n",
      "Eval &               Approach &    p &  map & retr &     p &   map &  retr &     p &   map &  retr \\\\\n",
      "\\midrule\n",
      "Appr &   exact:stns+alts+plcs & 0.33 & 0.72 & 0.79 &    -- &    -- &    -- &    -- &    -- &    -- \\\\\n",
      "Appr & partial:stns+alts+plcs & 0.32 & 0.73 & 0.80 &  0.21 &  0.61 &  0.81 &  0.18 &  0.49 &  0.82 \\\\\n",
      "Appr &   deezy:stns+alts+plcs & 0.29 & 0.71 & 0.80 &  0.19 &  0.71 &   0.8 &  0.18 &  0.71 &   0.8 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "==========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dRenameCombs = {\"stations\":\"stns\", \"stations+alts\":\"stns+alts\", \"stations+places+alts\":\"stns+alts+plcs\"}\n",
    "\n",
    "# Options:\n",
    "devtest_settings = [\"test\"]\n",
    "cr_approaches = [\"perfect_match\", \"partial_match\", \"deezy_match\"]\n",
    "ncand_options = [1, 3, 5]\n",
    "combinations = [[\"stations\"], [\"stations\", \"alts\"], [\"stations\", \"places\", \"alts\"]]\n",
    "\n",
    "for setting in devtest_settings:\n",
    "    for comb in combinations:\n",
    "                \n",
    "        print(\"==========================================\")\n",
    "        print(\"Split:\", setting)\n",
    "        print(\"Combination:\", comb)\n",
    "        \n",
    "        eval_results = []\n",
    "        for approach in cr_approaches:\n",
    "            appr_results = []\n",
    "            \n",
    "            for num_candidates in ncand_options:\n",
    "                \n",
    "                test_df = pd.read_pickle(\"../processed/resolution/candranking_\" + approach + \"_\" + setting + str(num_candidates) + \".pkl\")\n",
    "\n",
    "                # Get relevant columns from dataframe:\n",
    "                relv_columns = []\n",
    "                for c in comb:\n",
    "                    relv_columns.append(\"cr_\" + approach + \"_\" + c)\n",
    "\n",
    "                exact_station = True\n",
    "                if comb == [\"stations\", \"places\", \"alts\"]:\n",
    "                    exact_station = False\n",
    "                    \n",
    "                # Report performance:\n",
    "                p = test_df.apply(lambda row: eval_methods.pAt(row, approach, relv_columns, exact_station), axis=1).mean()\n",
    "                mapAt = test_df.apply(lambda row: eval_methods.avgP(row, approach, relv_columns, exact_station), axis=1).mean()\n",
    "                isRetrieved = test_df.apply(lambda row: eval_methods.isRetrieved(row, approach, relv_columns, exact_station), axis=1).mean()\n",
    "                \n",
    "                # Perfect match always returns just candidates where nv=1:\n",
    "                if approach == \"perfect_match\" and num_candidates  > 1:\n",
    "                    appr_results += [np.nan, np.nan, np.nan]\n",
    "                else:\n",
    "                    appr_results += [p, mapAt, isRetrieved]\n",
    "            \n",
    "            annotation = \"Strict\" if exact_station else \"Appr\"\n",
    "            approach_renamed = approach.split(\"_\")[0]\n",
    "            if approach_renamed == \"perfect\":\n",
    "                approach_renamed = \"exact\"\n",
    "            \n",
    "            eval_results.append([annotation, approach_renamed + \":\" + dRenameCombs[\"+\".join(comb)]] + appr_results)\n",
    "            \n",
    "        cr_eval_df = pd.DataFrame(eval_results, columns = [\"Eval\", \"Approach\", \"p\", \"map\", \"retr\", \"p\", \"map\", \"retr\", \"p\", \"map\", \"retr\"])\n",
    "        cr_eval_df = cr_eval_df.round(2)\n",
    "        cr_eval_df = cr_eval_df.fillna(\"--\")\n",
    "        print(cr_eval_df.to_latex(index=False))\n",
    "        print(\"==========================================\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate entity resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gazetteer\n",
    "gazetteer_df = pd.read_csv(\"../processed/wikidata/gb_gazetteer.csv\", header=0, index_col=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrllll}\n",
      "\\toprule\n",
      "                         Approach &  PStr & PAppr & Acc@1km & Acc@5km & Acc@10km \\\\\n",
      "\\midrule\n",
      "            skyline (deezy, nv=1) &  0.73 &     - &       - &       - &        - \\\\\n",
      "  string confidence (deezy, nv=1) &  0.66 &  0.69 &    0.77 &    0.84 &     0.85 \\\\\n",
      "wikipedia relevance (deezy, nv=1) &  0.10 &  0.16 &    0.54 &     0.8 &     0.81 \\\\\n",
      " semantic coherence (deezy, nv=1) &  0.30 &  0.32 &    0.58 &    0.78 &     0.79 \\\\\n",
      "            RankLib (deezy, nv=1) &  0.68 &   0.7 &    0.79 &    0.85 &     0.86 \\\\\n",
      "         SVM simple (deezy, nv=1) &  0.68 &  0.71 &     0.8 &    0.86 &     0.86 \\\\\n",
      "        SVM refined (deezy, nv=1) &  0.67 &   0.7 &    0.79 &    0.86 &     0.86 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\\begin{tabular}{lrllll}\n",
      "\\toprule\n",
      "                         Approach &  PStr & PAppr & Acc@1km & Acc@5km & Acc@10km \\\\\n",
      "\\midrule\n",
      "            skyline (deezy, nv=5) &  0.73 &     - &       - &       - &        - \\\\\n",
      "  string confidence (deezy, nv=5) &  0.66 &  0.68 &    0.77 &    0.85 &     0.85 \\\\\n",
      "wikipedia relevance (deezy, nv=5) &  0.06 &  0.11 &    0.42 &    0.65 &     0.65 \\\\\n",
      " semantic coherence (deezy, nv=5) &  0.25 &  0.26 &    0.45 &    0.61 &     0.62 \\\\\n",
      "            RankLib (deezy, nv=5) &  0.68 &  0.71 &    0.79 &    0.86 &     0.87 \\\\\n",
      "         SVM simple (deezy, nv=5) &  0.67 &  0.68 &    0.76 &    0.82 &     0.82 \\\\\n",
      "        SVM refined (deezy, nv=5) &  0.69 &  0.72 &     0.8 &    0.86 &     0.87 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cr_approaches = [\"deezy_match\"]\n",
    "ncand_options = [1, 5]\n",
    "\n",
    "# Dictionary of shorter names for the approaches:\n",
    "approachList = [\"skyline\", \"candrank_most_confident_1\", \"candrank_most_confident_2\", \"candrank_most_confident_3\", \"candrank_most_confident_4\", \"candrank_most_confident_5\", \"wikipedia_most_relevant\", \"semantically_most_similar\", \"ranklib_1\", \"ranklib_2\", \"ranklib_3\", \"ranklib_4\", \"ranklib_5\", \"our_method_all\", \"our_method_comb\"]\n",
    "\n",
    "for candrank_method in cr_approaches:\n",
    "    for num_candidates in ncand_options:\n",
    "        \n",
    "        dApproachNames = {\"candrank_most_confident\":\"string confidence\", \"wikipedia_most_relevant\":\"wikipedia relevance\", \"semantically_most_similar\":\"semantic coherence\", \"our_method_all\":\"SVM simple\", \"our_method_comb\":\"SVM refined\", \"skyline\": \"skyline\",\"ranklib\":\"RankLib\"}\n",
    "        \n",
    "        results_test_df = pd.read_pickle(\"../processed/resolution/resolved_\" + candrank_method + \"_test\" + str(num_candidates) + \".pkl\")\n",
    "        \n",
    "        eval_results = []\n",
    "        for topres_approach in approachList:\n",
    "\n",
    "            acc_at = (np.nan, np.nan, np.nan)\n",
    "            exact_acc_approx = np.nan\n",
    "\n",
    "            exact_acc_strict = eval_methods.topres_exactmetrics(results_test_df, topres_approach, True)\n",
    "\n",
    "            if topres_approach != \"skyline\":\n",
    "                acc_at = eval_methods.topres_distancemetrics(gazetteer_df, results_test_df, topres_approach, False)\n",
    "                exact_acc_approx = eval_methods.topres_exactmetrics(results_test_df, topres_approach, False)\n",
    "\n",
    "            eval_results.append([topres_approach, exact_acc_strict, exact_acc_approx, acc_at[0], acc_at[1], acc_at[2]])\n",
    "\n",
    "        tr_eval_df = pd.DataFrame(eval_results, columns = [\"Approach\", \"PStr\", \"PAppr\", \"Acc@1km\", \"Acc@5km\", \"Acc@10km\"])\n",
    "        \n",
    "        # Merge candrank and ranklib averages:\n",
    "        cand_rank_merged = tr_eval_df[tr_eval_df['Approach'].str.contains(\"candrank_most_confident\")].mean(axis=0).to_dict()\n",
    "        cand_rank_merged[\"Approach\"] = \"candrank_most_confident\"\n",
    "        ranklib_merged = tr_eval_df[tr_eval_df['Approach'].str.contains(\"ranklib\")].mean(axis=0).to_dict()\n",
    "        ranklib_merged[\"Approach\"] = \"ranklib\"\n",
    "        tr_eval_df = tr_eval_df.append(cand_rank_merged, ignore_index=True)\n",
    "        tr_eval_df = tr_eval_df.append(ranklib_merged, ignore_index=True)\n",
    "        tr_eval_df = tr_eval_df[~tr_eval_df.Approach.str.startswith(\"candrank_most_confident_\")]\n",
    "        tr_eval_df = tr_eval_df[~tr_eval_df.Approach.str.startswith(\"ranklib_\")]\n",
    "        \n",
    "        # Simplify method name and reorder rows:\n",
    "        tr_eval_df[\"Approach\"] = tr_eval_df[\"Approach\"].replace(dApproachNames)\n",
    "        tr_eval_df[\"Approach\"] = tr_eval_df[\"Approach\"] + \" (\" + candrank_method.split(\"_\")[0] + \", nv=\" + str(num_candidates) + \")\"\n",
    "        tr_eval_df = tr_eval_df.reset_index()\n",
    "        row_index = pd.Series([0, 5, 1, 2, 6, 3, 4])\n",
    "        tr_eval_df = tr_eval_df.iloc[row_index, :]\n",
    "        tr_eval_df = tr_eval_df.drop(columns=[\"index\"])\n",
    "        \n",
    "        # Round values, and print the latex table:\n",
    "        tr_eval_df = tr_eval_df.round(2)\n",
    "        tr_eval_df = tr_eval_df.fillna(\"-\")\n",
    "        print(tr_eval_df.to_latex(index=False))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37deezy)",
   "language": "python",
   "name": "py37deezy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
