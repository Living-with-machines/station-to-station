{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from tools import eval_methods, selection_methods\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate candidate selection and ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options:\n",
    "devtest_settings = [\"test\"]\n",
    "cr_approaches = [\"deezy_match\", \"partial_match\", \"perfect_match\"]\n",
    "ncand_options = [1, 3, 5]\n",
    "exact_options = [True, False]\n",
    "\n",
    "# -----------------------------------\n",
    "# Evaluate the different candrank experiments\n",
    "for setting in devtest_settings:\n",
    "    for approach in cr_approaches:\n",
    "        for num_candidates in ncand_options:\n",
    "            for exact_station in exact_options:\n",
    "                print(\"==========================================\")\n",
    "                print(\"Split:\", setting)\n",
    "                print(\"Approach:\", approach)\n",
    "                print(\"Num candidates:\", num_candidates)\n",
    "                print(\"Strict eval:\", exact_station)\n",
    "\n",
    "                test_df = pd.read_pickle(\"../processed/resolution/candranking_\" + approach + \"_\" + setting + str(num_candidates) + \".pkl\")\n",
    "\n",
    "                combinations = [[\"stations\"], [\"stations\", \"alts\"], [\"stations\", \"places\", \"alts\"]]\n",
    "                candrank_approaches = [x.replace(\"cr_\", \"\").replace(\"_stations\", \"\") for x in test_df if x.startswith(\"cr_\") and x.endswith(\"stations\")]\n",
    "\n",
    "                eval_results = []\n",
    "                for combination in combinations:\n",
    "                    for approach in candrank_approaches:\n",
    "\n",
    "                        # Get relevant columns from dataframe:\n",
    "                        relv_columns = []\n",
    "                        for c in combination:\n",
    "                            relv_columns.append(\"cr_\" + approach + \"_\" + c)\n",
    "\n",
    "                        # Report performance:\n",
    "                        p = test_df.apply(lambda row: eval_methods.pAt(row, approach, relv_columns, exact_station), axis=1).mean()\n",
    "                        mapAt = test_df.apply(lambda row: eval_methods.avgP(row, approach, relv_columns, exact_station), axis=1).mean()\n",
    "                        isRetrieved = test_df.apply(lambda row: eval_methods.isRetrieved(row, approach, relv_columns, exact_station), axis=1).mean()\n",
    "                        eval_results.append([approach + \": \" + \"+\".join(combination), p, mapAt, isRetrieved])\n",
    "\n",
    "                annotation = \"strict\" if exact_station else \"appr\"\n",
    "                cr_eval_df = pd.DataFrame(eval_results, columns = [\"Approach:\" + annotation, \"p@\" + str(num_candidates), \"map@\" + str(num_candidates), \"retr@\" + str(num_candidates)])\n",
    "                cr_eval_df.round(3)\n",
    "                print(cr_eval_df.round(2).to_latex(index=False))\n",
    "                print(\"==========================================\")\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate entity resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gazetteer\n",
    "gazetteer_df = pd.read_csv(\"../processed/wikidata/gb_gazetteer.csv\", header=0, index_col=0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cr_approaches = [\"deezy_match\", \"partial_match\", \"perfect_match\"]\n",
    "ncand_options = [1, 3, 5]\n",
    "\n",
    "for candrank_method in cr_approaches:\n",
    "    for num_candidates in ncand_options:\n",
    "        print(candrank_method, num_candidates)\n",
    "\n",
    "        results_test_df = pd.read_pickle(\"../processed/resolution/resolved_\" + candrank_method + \"_test\" + str(num_candidates) + \".pkl\")\n",
    "\n",
    "        eval_results = []\n",
    "        dApproachNames = {\"candrank_most_confident\":\"string confidence\", \"wikipedia_most_relevant\":\"wikipedia relevance\", \"semantically_most_similar\":\"semantic coherence\", \"our_method_all\":\"SVM simple\", \"our_method_comb\":\"SVM combined\", \"skyline\": \"skyline\",\"ranklib\":\"ranklib all features\"}\n",
    "        for topres_approach in [\"skyline\", \"candrank_most_confident\", \"wikipedia_most_relevant\", \"semantically_most_similar\",\"ranklib\", \"our_method_all\", \"our_method_comb\"]:\n",
    "            print(topres_approach, candrank_method, \"(numCands: \" + str(num_candidates) + \")\")\n",
    "\n",
    "            acc_at = (np.nan, np.nan, np.nan)\n",
    "            exact_acc_approx = np.nan\n",
    "\n",
    "            exact_acc_strict = eval_methods.topres_exactmetrics(results_test_df, topres_approach, True)\n",
    "\n",
    "            if topres_approach != \"skyline\":\n",
    "                acc_at = eval_methods.topres_distancemetrics(gazetteer_df, results_test_df, topres_approach, False)\n",
    "                exact_acc_approx = eval_methods.topres_exactmetrics(results_test_df, topres_approach, False)\n",
    "\n",
    "            eval_results.append([dApproachNames[topres_approach], exact_acc_strict, exact_acc_approx, acc_at[0], acc_at[1], acc_at[2]])\n",
    "\n",
    "        tr_eval_df = pd.DataFrame(eval_results, columns = [\"Approach\", \"PrecStr\", \"PrecAppr\", \"Acc@1km\", \"Acc@5km\", \"Acc@10km\"])\n",
    "\n",
    "        print()\n",
    "        tr_eval_df = tr_eval_df.round(2)\n",
    "        tr_eval_df = tr_eval_df.fillna(\"-\")\n",
    "        print(tr_eval_df.to_latex(index=False))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37deezy)",
   "language": "python",
   "name": "py37deezy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
