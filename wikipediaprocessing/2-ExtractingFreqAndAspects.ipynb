{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk,re, pickle, os, json\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from urllib.parse import quote\n",
    "import multiprocessing as mp\n",
    "from wikimapper import WikiMapper\n",
    "import wptools\n",
    "\n",
    "mapper = WikiMapper(\"/resources/wikidata2wikipedia/index_enwiki-20190420.db\")\n",
    "\n",
    "def get_all_ngrams(text,ngram_up_to):\n",
    "    \n",
    "    global all_mentions\n",
    "\n",
    "    # removing html tags\n",
    "    content = re.sub('<[^>]+>', '', text)\n",
    "    \n",
    "    #word tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    all_ngrams = [\" \".join(x) for n in range(1,ngram_up_to+1) for x in nltk.ngrams(tokens,n)]        \n",
    "     \n",
    "    all_ngrams = [x for x in all_ngrams if x in all_mentions]\n",
    "    \n",
    "    return all_ngrams\n",
    "\n",
    "def clean_page(page):\n",
    "    \n",
    "    global ngram_up_to\n",
    "    \n",
    "    entities = [x for x in page.findAll(\"a\") if x.has_attr(\"href\")]\n",
    "     \n",
    "    page_text = page.text\n",
    "    all_ngrams = get_all_ngrams(page_text,ngram_up_to)\n",
    "\n",
    "    content_ngrams= Counter(all_ngrams)\n",
    "    \n",
    "    box_mentions = Counter([x.text for x in entities])\n",
    "    box_entities = Counter([x[\"href\"] for x in entities])\n",
    "        \n",
    "    mentions_dict = {x:[] for x in box_mentions}\n",
    "    for e in entities:\n",
    "        mentions_dict[e.text].append(e[\"href\"])\n",
    "    \n",
    "    mentions_dict = {x:Counter(y) for x,y in mentions_dict.items()} \n",
    "    \n",
    "    return [box_mentions,content_ngrams,box_entities,mentions_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sections (page):\n",
    "    page = page.text.strip().split(\"\\n\")\n",
    "    sections = {\"Main\":{\"order\":1,\"content\":[]}}\n",
    "    dict_key = \"Main\"\n",
    "    ct = 1\n",
    "    for line in page:\n",
    "        if not \"Section::::\" in line:\n",
    "            sections[dict_key][\"content\"].append(line)\n",
    "        else:\n",
    "            ct+=1\n",
    "            dict_key = line.replace(\"Section::::\",\"\")[:-1]\n",
    "            sections[dict_key] = {\"order\":ct,\"content\":[]}\n",
    "            \n",
    "    sections = {x:y for x,y in sections.items() if len(y[\"content\"])>0}\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(doc):\n",
    "    content = open(proessed_docs+folder+\"/\"+doc).read()\n",
    "    content = BeautifulSoup(content).findAll(\"doc\")\n",
    "    pages = []\n",
    "    for page in content:\n",
    "        title = page[\"title\"]\n",
    "        wikidata_id = mapper.title_to_id(title.replace(\" \",\"_\"))\n",
    "        if wikidata_id is None:\n",
    "            wikidata_id = wptools.page('Indiana Jones (character)',silent=True).get_wikidata().data[\"wikibase\"]\n",
    "        #title = quote(page[\"title\"]).replace(\"/\",\"-\")\n",
    "        sections = {\"wikidata_id\":wikidata_id,\"title\":title,\"sections\": get_sections(page)}\n",
    "        r = [title]+ clean_page(page) + [sections]\n",
    "        pages.append([r])\n",
    "    return pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the set of all possible mentions \n",
    "\n",
    "with open(\"/resources/wikipedia/extractedResources/all_mentions.pickle\", \"rb\") as f:\n",
    "    all_mentions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Process ForkPoolWorker-698:\n",
      "Process ForkPoolWorker-688:\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/anaconda/envs/py37linking/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-697:\n",
      "Process ForkPoolWorker-699:\n",
      "Process ForkPoolWorker-692:\n",
      "Process ForkPoolWorker-689:\n",
      "Process ForkPoolWorker-700:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-694:\n",
      "  File \"/data/anaconda/envs/py37linking/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkPoolWorker-691:\n",
      "Process ForkPoolWorker-693:\n",
      "Process ForkPoolWorker-690:\n",
      "Process ForkPoolWorker-695:\n",
      "  File \"/data/anaconda/envs/py37linking/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/anaconda/envs/py37linking/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-696:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n"
     ]
    }
   ],
   "source": [
    "# the output already used before, coming from WikiExtractor\n",
    "\n",
    "proessed_docs = \"/resources/wikipedia/processedWiki/\"\n",
    "\n",
    "ngram_up_to = 3\n",
    "\n",
    "# again, the number of cpu\n",
    "N= mp.cpu_count()-2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    step = 1\n",
    "\n",
    "    for folder in os.listdir(proessed_docs):\n",
    "        \n",
    "            with mp.Pool(processes = N) as p:\n",
    "                res = p.map(process_doc, os.listdir(proessed_docs+folder))\n",
    "\n",
    "            res = [y for x in res for y in x]\n",
    "            \n",
    "            # separating frequency counts from aspects\n",
    "            freq_res = [x[0][:-1] for x in res]\n",
    "            sections = [x[0][-1] for x in res]\n",
    "            \n",
    "            # saving sections independently\n",
    "            for sect in sections:\n",
    "                title = sect[\"title\"]\n",
    "                wikidata_id = sect[\"wikidata_id\"]\n",
    "                if wikidata_id is not None:\n",
    "                    s = sect[\"sections\"]\n",
    "                    try:\n",
    "                        with open('/resources/wikipedia/extractedResources/Pages/'+wikidata_id+\".json\", 'w') as fp:\n",
    "                            json.dump(s, fp)\n",
    "                    except OSError as e:\n",
    "                        print (e)\n",
    "                        continue\n",
    "                else:\n",
    "                    print (\"Missing:\",title)\n",
    "            # storing counts, still divided in folders       \n",
    "            with open('/resources/wikipedia/extractedResources/Store-Counts/'+str(step)+\".json\", 'w') as fp:\n",
    "                json.dump(freq_res, fp)\n",
    "            \n",
    "            print(\"Done %s folders over %s\" % (step, len(os.listdir(proessed_docs))))\n",
    "            step+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}