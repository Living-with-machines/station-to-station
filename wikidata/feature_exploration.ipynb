{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Wikidata places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import pandas as pd\n",
    "import pydash\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the languages of the alternate names (Wikimedia language codes:\n",
    "# https://www.wikidata.org/wiki/Help:Wikimedia_language_codes/lists/all)\n",
    "languages = ['en', 'cy', 'sco', 'gd', 'ga', 'kw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Parse a Wikidata record:\n",
    "# Function that takes a Wikidata entry (record) as input,\n",
    "# parses it, and returns a dictionary.\n",
    "# ==========================================\n",
    "\n",
    "def parse_record(record):\n",
    "    # Wikidata ID:\n",
    "    wikidata_id = record['id']\n",
    "\n",
    "    # ==========================================\n",
    "    # Place description and definition\n",
    "    # ==========================================\n",
    "\n",
    "    # Main label:\n",
    "    english_label = pydash.get(record, 'labels.en.value')\n",
    "\n",
    "    # Location is instance of\n",
    "    instance_of_dict = pydash.get(record, 'claims.P31')\n",
    "    instance_of = None\n",
    "    if instance_of_dict:\n",
    "        instance_of = [pydash.get(r, 'mainsnak.datavalue.value.id') for r in instance_of_dict]\n",
    "\n",
    "    # Descriptions in English:\n",
    "    description_set = set()\n",
    "    descriptions = pydash.get(record, 'descriptions')\n",
    "    for x in descriptions:\n",
    "        if x == 'en' or x.startswith('en-'):\n",
    "            description_set.add(pydash.get(descriptions[x], 'value'))\n",
    "\n",
    "    # Aliases and labels:\n",
    "    aliases = pydash.get(record, 'aliases')\n",
    "    labels = pydash.get(record, 'labels')\n",
    "    alias_dict = dict()\n",
    "    for x in aliases:\n",
    "        if x in languages or x.startswith('en-'):\n",
    "            for y in aliases[x]:\n",
    "                if \"value\" in y:\n",
    "                    if not y[\"value\"].isupper() and not y[\"value\"].islower() and any(x.isalpha() for x in y[\"value\"]):\n",
    "                        if x in alias_dict:\n",
    "                            if not y[\"value\"] in alias_dict[x]:\n",
    "                                alias_dict[x].append(y[\"value\"])\n",
    "                        else:\n",
    "                            alias_dict[x] = [y[\"value\"]]\n",
    "    for x in labels:\n",
    "        if x in languages or x.startswith('en-'):\n",
    "            if \"value\" in labels[x]:\n",
    "                if not labels[x][\"value\"].isupper() and not labels[x][\"value\"].islower() and any(z.isalpha() for z in labels[x][\"value\"]):\n",
    "                    if x in alias_dict:\n",
    "                        if not labels[x][\"value\"] in alias_dict[x]:\n",
    "                            alias_dict[x].append(labels[x][\"value\"])\n",
    "                    else:\n",
    "                        alias_dict[x] = [labels[x][\"value\"]]\n",
    "\n",
    "    # Native label\n",
    "    nativelabel_dict = pydash.get(record, 'claims.P1705')\n",
    "    nativelabel = None\n",
    "    if nativelabel_dict:\n",
    "        nativelabel = [pydash.get(c, 'mainsnak.datavalue.value.text') for c in nativelabel_dict]\n",
    "\n",
    "    # ==========================================\n",
    "    # Geographic and demographic information\n",
    "    # ==========================================\n",
    "\n",
    "    # Population at: dictionary of year-population pairs\n",
    "    population_dump = pydash.get(record, 'claims.P1082')\n",
    "    population_dict = dict()\n",
    "    if population_dump:\n",
    "        for ppl in population_dump:\n",
    "            pop_amount = pydash.get(ppl, 'mainsnak.datavalue.value.amount')\n",
    "            pop_time = pydash.get(ppl, 'qualifiers.P585[0].datavalue.value.time')\n",
    "            pop_time = \"UNKNOWN\" if not pop_time else pop_time\n",
    "            population_dict[pop_time] = pop_amount\n",
    "\n",
    "    # Area of location\n",
    "    dict_area_units = {'Q712226' : 'square kilometre',\n",
    "               'Q2737347': 'square millimetre',\n",
    "               'Q2489298': 'square centimetre',\n",
    "               'Q35852': 'hectare',\n",
    "               'Q185078': 'are',\n",
    "               'Q25343': 'square metre'}\n",
    "\n",
    "    area_loc = pydash.get(record, 'claims.P2046[0].mainsnak.datavalue.value')\n",
    "    area = None\n",
    "    if area_loc:\n",
    "        try:\n",
    "            if area_loc.get(\"unit\"):\n",
    "                area = (area_loc.get(\"amount\"), dict_area_units.get(area_loc.get(\"unit\").split(\"/\")[-1]))\n",
    "        except:\n",
    "            area = None\n",
    "\n",
    "    # ==========================================\n",
    "    # Historical information\n",
    "    # ==========================================\n",
    "\n",
    "    # Historical counties\n",
    "    hcounties_dict = pydash.get(record, 'claims.P7959')\n",
    "    hcounties = []\n",
    "    if hcounties_dict:\n",
    "        hcounties = [pydash.get(hc, 'mainsnak.datavalue.value.id') for hc in hcounties_dict]\n",
    "\n",
    "    # Date of official opening (e.g. https://www.wikidata.org/wiki/Q2011)\n",
    "    date_opening = pydash.get(record, 'claims.P1619[0].mainsnak.datavalue.value.time')\n",
    "\n",
    "    # Date of official closing\n",
    "    date_closing = pydash.get(record, 'claims.P3999[0].mainsnak.datavalue.value.time')\n",
    "\n",
    "    # Inception: date or point in time when the subject came into existence as defined\n",
    "    inception_date = pydash.get(record, 'claims.P571[0].mainsnak.datavalue.value.time')\n",
    "\n",
    "    # Dissolved, abolished or demolished: point in time at which the subject ceased to exist\n",
    "    dissolved_date = pydash.get(record, 'claims.P576[0].mainsnak.datavalue.value.time')\n",
    "\n",
    "    # Follows...: immediately prior item in a series of which the subject is a part: e.g. Vanuatu follows New Hebrides\n",
    "    follows_dict = pydash.get(record, 'claims.P155')\n",
    "    follows = []\n",
    "    if follows_dict:\n",
    "        for f in follows_dict:\n",
    "            follows.append(pydash.get(f, 'mainsnak.datavalue.value.id'))\n",
    "\n",
    "    # Replaces...: item replaced: e.g. New Hebrides is replaced by \n",
    "    replaces_dict = pydash.get(record, 'claims.P1365')\n",
    "    replaces = []\n",
    "    if replaces_dict:\n",
    "        for r in replaces_dict:\n",
    "            replaces.append(pydash.get(r, 'mainsnak.datavalue.value.id'))\n",
    "\n",
    "    # Heritage designation\n",
    "    heritage_designation = pydash.get(record, 'claims.P1435[0].mainsnak.datavalue.value.id')\n",
    "\n",
    "    # ==========================================\n",
    "    # Neighbouring or part-of locations\n",
    "    # ==========================================\n",
    "\n",
    "    # Located in adminitrative territorial entities (Wikidata ID)\n",
    "    adm_regions_dict = pydash.get(record, 'claims.P131')\n",
    "    adm_regions = dict()\n",
    "    if adm_regions_dict:\n",
    "        for r in adm_regions_dict:\n",
    "            regname = pydash.get(r, 'mainsnak.datavalue.value.id')\n",
    "            if regname:\n",
    "                entity_start_time = pydash.get(r, 'qualifiers.P580[0].datavalue.value.time')\n",
    "                entity_end_time = pydash.get(r, 'qualifiers.P582[0].datavalue.value.time')\n",
    "                adm_regions[regname] = (entity_start_time, entity_end_time)\n",
    "\n",
    "    # Country: sovereign state of this item\n",
    "    country_dict = pydash.get(record, 'claims.P17')\n",
    "    countries = dict()\n",
    "    if country_dict:\n",
    "        for r in country_dict:\n",
    "            countryname = pydash.get(r, 'mainsnak.datavalue.value.id')\n",
    "            if countryname:\n",
    "                entity_start_time = pydash.get(r, 'qualifiers.P580[0].datavalue.value.time')\n",
    "                entity_end_time = pydash.get(r, 'qualifiers.P582[0].datavalue.value.time')\n",
    "                countries[countryname] = (entity_start_time, entity_end_time)\n",
    "\n",
    "    # Continents (Wikidata ID)\n",
    "    continent_dict = pydash.get(record, 'claims.P30')\n",
    "    continents = None\n",
    "    if continent_dict:\n",
    "        continents = [pydash.get(r, 'mainsnak.datavalue.value.id') for r in continent_dict]\n",
    "\n",
    "    # Location is capital of\n",
    "    capital_of_dict = pydash.get(record, 'claims.P1376')\n",
    "    capital_of = None\n",
    "    if capital_of_dict:\n",
    "        capital_of = [pydash.get(r, 'mainsnak.datavalue.value.id') for r in capital_of_dict]\n",
    "\n",
    "    # Shares border with:\n",
    "    shares_border_dict = pydash.get(record, 'claims.P47')\n",
    "    borders = []\n",
    "    if shares_border_dict:\n",
    "        borders = [pydash.get(t, 'mainsnak.datavalue.value.id') for t in shares_border_dict]\n",
    "\n",
    "    # Nearby waterbodies (Wikidata ID)\n",
    "    near_water_dict = pydash.get(record, 'claims.P206')\n",
    "    near_water = None\n",
    "    if near_water_dict:\n",
    "        near_water = [pydash.get(r, 'mainsnak.datavalue.value.id') for r in near_water_dict]\n",
    "\n",
    "    # Nearby waterbodies (Wikidata ID)\n",
    "    part_of_dict = pydash.get(record, 'claims.P361')\n",
    "    part_of = None\n",
    "    if part_of_dict:\n",
    "        part_of = [pydash.get(r, 'mainsnak.datavalue.value.id') for r in part_of_dict] \n",
    "    \n",
    "\n",
    "    # ==========================================\n",
    "    # Coordinates\n",
    "    # ==========================================\n",
    "\n",
    "    # Latitude and longitude:\n",
    "    latitude = pydash.get(record, 'claims.P625[0].mainsnak.datavalue.value.latitude')\n",
    "    longitude = pydash.get(record, 'claims.P625[0].mainsnak.datavalue.value.longitude')\n",
    "    if latitude and longitude:\n",
    "        latitude = round(latitude, 6)\n",
    "        longitude = round(longitude, 6)\n",
    "\n",
    "    # ==========================================\n",
    "    # External data resources IDs\n",
    "    # ==========================================\n",
    "\n",
    "    # English Wikipedia title:\n",
    "    wikititle = pydash.get(record, 'sitelinks.enwiki.title')\n",
    "\n",
    "    # Geonames ID\n",
    "    geonamesID_dict = pydash.get(record, 'claims.P1566')\n",
    "    geonamesIDs = None\n",
    "    if geonamesID_dict:\n",
    "        geonamesIDs = [pydash.get(gn, 'mainsnak.datavalue.value') for gn in geonamesID_dict]\n",
    "\n",
    "    # TOID: TOpographic IDentifier assigned by the Ordnance Survey to identify a feature in Great Britain\n",
    "    toID_dict = pydash.get(record, 'claims.P3120')\n",
    "    toIDs = None\n",
    "    if toID_dict:\n",
    "        toIDs = [pydash.get(t, 'mainsnak.datavalue.value') for t in toID_dict]\n",
    "\n",
    "    # British History Online VCH ID: identifier of a place, in the British History Online digitisation of the Victoria County History\n",
    "    vchID_dict = pydash.get(record, 'claims.P3628')\n",
    "    vchIDs = None\n",
    "    if vchID_dict:\n",
    "        vchIDs = [pydash.get(t, 'mainsnak.datavalue.value') for t in vchID_dict]\n",
    "\n",
    "    # Vision of Britain place ID: identifier of a place\n",
    "    vob_placeID_dict = pydash.get(record, 'claims.P3616')\n",
    "    vob_placeIDs = None\n",
    "    if vob_placeID_dict:\n",
    "        vob_placeIDs = [pydash.get(vobid, 'mainsnak.datavalue.value') for vobid in vob_placeID_dict]\n",
    "\n",
    "    # Vision of Britain unit ID: identifier of an administrative unit\n",
    "    vob_unitID_dict = pydash.get(record, 'claims.P3615')\n",
    "    vob_unitIDs = None\n",
    "    if vob_unitID_dict:\n",
    "        vob_unitIDs = dict()\n",
    "        for vobid in vob_unitID_dict:\n",
    "            unit_id = pydash.get(vobid, 'mainsnak.datavalue.value')\n",
    "            parish_name = pydash.get(vobid, 'qualifiers.P1810[0].datavalue.value')\n",
    "            vob_unitIDs[unit_id] = parish_name\n",
    "\n",
    "    # Identifier for a place in the Historical Gazetteer of England's Place Names website\n",
    "    epns_dict = pydash.get(record, 'claims.P3627')\n",
    "    epns = None\n",
    "    if epns_dict:\n",
    "        epns = [pydash.get(p, 'mainsnak.datavalue.value') for p in epns_dict]\n",
    "\n",
    "    # Identifier in the Getty Thesaurus of Geographic Names\n",
    "    getty_dict = pydash.get(record, 'claims.P1667')\n",
    "    getty = None\n",
    "    if getty_dict:\n",
    "        getty = [pydash.get(p, 'mainsnak.datavalue.value') for p in getty_dict]\n",
    "\n",
    "    # OS grid reference (Wikidata ID)\n",
    "    os_grid_ref = pydash.get(record, 'claims.P613[0].mainsnak.datavalue.value')\n",
    "\n",
    "    # ==========================================\n",
    "    # Street-related properties\n",
    "    # ==========================================\n",
    "\n",
    "    # Street connects with\n",
    "    connectswith_dict = pydash.get(record, 'claims.P2789')\n",
    "    connectswith = None\n",
    "    if connectswith_dict:\n",
    "        connectswith = [pydash.get(c, 'mainsnak.datavalue.value.id') for c in connectswith_dict]\n",
    "\n",
    "    # Street address\n",
    "    street_address = pydash.get(record, 'claims.P6375[0].mainsnak.datavalue.value.text')\n",
    "\n",
    "    # Located on street\n",
    "    street_located = pydash.get(record, 'claims.P669[0].mainsnak.datavalue.value.id')\n",
    "\n",
    "    # Postal code\n",
    "    postal_code_dict = pydash.get(record, 'claims.P281')\n",
    "    postal_code = None\n",
    "    if postal_code_dict:\n",
    "        postal_code = [pydash.get(c, 'mainsnak.datavalue.value') for c in postal_code_dict]\n",
    "\n",
    "    # ==========================================\n",
    "    # Rail-related properties\n",
    "    # ==========================================\n",
    "\n",
    "    # Adjacent stations\n",
    "    adjacent_stations = None\n",
    "    adj_st_dump = pydash.get(record, 'claims.P197')\n",
    "    if adj_st_dump:\n",
    "        adjacent_stations = [pydash.get(adj_st, 'mainsnak.datavalue.value.id') for adj_st in adj_st_dump]\n",
    "\n",
    "    # UK railway station code\n",
    "    ukrailcode_dict = pydash.get(record, 'claims.P4755')\n",
    "    ukrailcode = None\n",
    "    if ukrailcode_dict:\n",
    "        ukrailcode = [pydash.get(ukrid, 'mainsnak.datavalue.value') for ukrid in ukrailcode_dict]\n",
    "\n",
    "    # Connecting lines\n",
    "    connectline_dict = pydash.get(record, 'claims.P81')\n",
    "    connectline = None\n",
    "    if connectline_dict:\n",
    "        connectline = [pydash.get(conline, 'mainsnak.datavalue.value.id') for conline in connectline_dict]\n",
    "\n",
    "    # Owned by\n",
    "    ownedby_dict = pydash.get(record, 'claims.P127')\n",
    "    ownedby = None\n",
    "    if ownedby_dict:\n",
    "        ownedby = [pydash.get(conline, 'mainsnak.datavalue.value.id') for conline in ownedby_dict]\n",
    "\n",
    "    # Connecting service\n",
    "    connectservice_dict = pydash.get(record, 'claims.P1192')\n",
    "    connectservice = None\n",
    "    if connectservice_dict:\n",
    "        connectservice = [pydash.get(conline, 'mainsnak.datavalue.value.id') for conline in connectservice_dict]\n",
    "\n",
    "    # ==========================================\n",
    "    # Store records in a dictionary\n",
    "    # ==========================================\n",
    "    df_record = {'wikidata_id': wikidata_id, 'english_label': english_label,\n",
    "                 'instance_of': instance_of, 'description_set': description_set,\n",
    "                 'alias_dict': alias_dict, 'nativelabel': nativelabel,\n",
    "                 'population_dict': population_dict, 'area': area,\n",
    "                 'hcounties': hcounties, 'date_opening': date_opening,\n",
    "                 'date_closing': date_closing, 'inception_date': inception_date,\n",
    "                 'dissolved_date': dissolved_date, 'follows': follows,\n",
    "                 'replaces': replaces, 'adm_regions': adm_regions,\n",
    "                 'countries': countries, 'continents': continents,\n",
    "                 'capital_of': capital_of, 'borders': borders, 'near_water': near_water,\n",
    "                 'latitude': latitude, 'longitude': longitude, 'wikititle': wikititle,\n",
    "                 'geonamesIDs': geonamesIDs, 'toIDs': toIDs, 'vchIDs': vchIDs,\n",
    "                 'vob_placeIDs': vob_placeIDs, 'vob_unitIDs': vob_unitIDs,\n",
    "                 'epns': epns, 'os_grid_ref': os_grid_ref, 'connectswith': connectswith,\n",
    "                 'street_address': street_address, 'adjacent_stations': adjacent_stations,\n",
    "                 'ukrailcode': ukrailcode, 'connectline': connectline,\n",
    "                 'heritage_designation': heritage_designation, 'getty': getty,\n",
    "                 'street_located': street_located, 'postal_code': postal_code,\n",
    "                 'ownedby': ownedby, 'connectservice': connectservice\n",
    "                }\n",
    "    return df_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse just _one_ Wikidata record (from client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the wikidata ID of the entity you would like to\n",
    "# parse (e.g. \"Q609161\": https://www.wikidata.org/wiki/Q609161):\n",
    "wikidata_entity = \"Q3397374\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the source using a Wikidata client:\n",
    "from wikidata.client import Client\n",
    "client = Client()\n",
    "entity = client.get(wikidata_entity, load=True)\n",
    "record = entity.__dict__['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the record:\n",
    "parse_record(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the source of the wikidata entry:\n",
    "print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse all WikiData\n",
    "\n",
    "To run on the whole wikidata (warning: this can take days!):\n",
    "* Download a full Wikidata dump from [here](https://dumps.wikimedia.org/wikidatawiki/entities/latest-all.json.bz2).\n",
    "* Store it locally, change the path in `wikidata_bz2_path`.\n",
    "* Uncomment the following cell (\"Parse all WikiData\"), and run it.\n",
    "* Credits: This code is partially based on https://akbaritabar.netlify.app/how_to_use_a_wikidata_dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ==========================================\n",
    "# # Parse all WikiData\n",
    "# # ==========================================\n",
    "\n",
    "# wikidata_bz2_path = '/resources/wikidata/latest-all.json.bz2'\n",
    "\n",
    "# def wikidata(filename):\n",
    "#     with bz2.open(filename, mode='rt') as f:\n",
    "#         f.read(2) # skip first two bytes: \"{\\n\"\n",
    "#         for line in f:\n",
    "#             try:\n",
    "#                 yield json.loads(line.rstrip(',\\n'))\n",
    "#             except json.decoder.JSONDecodeError:\n",
    "#                 continue\n",
    "\n",
    "# df_record_all = pd.DataFrame(columns=['wikidata_id', 'english_label', 'instance_of', 'description_set', 'alias_dict', 'nativelabel', 'population_dict', 'area', 'hcounties', 'date_opening', 'date_closing', 'inception_date', 'dissolved_date', 'follows', 'replaces', 'adm_regions', 'countries', 'continents', 'capital_of', 'borders', 'near_water', 'latitude', 'longitude', 'wikititle', 'geonamesIDs', 'toIDs', 'vchIDs', 'vob_placeIDs', 'vob_unitIDs', 'epns', 'os_grid_ref', 'connectswith', 'street_address', 'adjacent_stations', 'ukrailcode', 'connectline'])\n",
    "\n",
    "# header=True\n",
    "# i = 0\n",
    "# for record in tqdm(wikidata(wikidata_bz2_path)):\n",
    "    \n",
    "#     # Only extract items with geographical coordinates (P625)\n",
    "#     if pydash.has(record, 'claims.P625'):\n",
    "        \n",
    "#         # ==========================================\n",
    "#         # Store records in a csv\n",
    "#         # ==========================================\n",
    "#         df_record = parse_record(record)\n",
    "#         df_record_all = df_record_all.append(df_record, ignore_index=True)\n",
    "#         i += 1\n",
    "#         if (i % 5000 == 0):\n",
    "#             pd.DataFrame.to_csv(df_record_all, path_or_buf='extracted/till_'+record['id']+'_item.csv')\n",
    "#             print('i = '+str(i)+' item '+record['id']+'  Done!')\n",
    "#             print('CSV exported')\n",
    "#             df_record_all = pd.DataFrame(columns=['wikidata_id', 'english_label', 'instance_of', 'description_set', 'alias_dict', 'nativelabel', 'population_dict', 'area', 'hcounties', 'date_opening', 'date_closing', 'inception_date', 'dissolved_date', 'follows', 'replaces', 'adm_regions', 'countries', 'continents', 'capital_of', 'borders', 'near_water', 'latitude', 'longitude', 'wikititle', 'geonamesIDs', 'toIDs', 'vchIDs', 'vob_placeIDs', 'vob_unitIDs', 'epns', 'os_grid_ref', 'connectswith', 'street_address', 'adjacent_stations', 'ukrailcode', 'connectline'])\n",
    "#         else:\n",
    "#             continue\n",
    "        \n",
    "#         ## Test on the first 100 records:\n",
    "#         # if i >= 100:\n",
    "#         #     break\n",
    "            \n",
    "# pd.DataFrame.to_csv(df_record_all, path_or_buf='extracted/final_csv_till_'+record['id']+'_item.csv')\n",
    "# print('i = '+str(i)+' item '+record['id']+'  Done!')\n",
    "# print('All items finished, final CSV exported!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37deezy)",
   "language": "python",
   "name": "py37deezy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
