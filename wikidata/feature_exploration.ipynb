{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Wikidata places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import pandas as pd\n",
    "import pydash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the languages of the alternate names (Wikimedia language codes:\n",
    "# https://www.wikidata.org/wiki/Help:Wikimedia_language_codes/lists/all)\n",
    "languages = ['en', 'cy', 'sco', 'gd', 'ga', 'kw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Parse a Wikidata record:\n",
    "# Function that takes a Wikidata entry (record) as input,\n",
    "# parses it, and returns a dictionary.\n",
    "# ==========================================\n",
    "\n",
    "def parse_record(record):\n",
    "    # Wikidata ID:\n",
    "    wikidata_id = record['id']\n",
    "\n",
    "    # ==========================================\n",
    "    # Place description and definition\n",
    "    # ==========================================\n",
    "\n",
    "    # Main label:\n",
    "    english_label = pydash.get(record, 'labels.en.value')\n",
    "\n",
    "    # Location is instance of\n",
    "    instance_of_dict = pydash.get(record, 'claims.P31')\n",
    "    instance_of = None\n",
    "    if instance_of_dict:\n",
    "        instance_of = [pydash.get(r, 'mainsnak.datavalue.value.id') for r in instance_of_dict]\n",
    "\n",
    "    # Descriptions in English:\n",
    "    description_set = set()\n",
    "    descriptions = pydash.get(record, 'descriptions')\n",
    "    for x in descriptions:\n",
    "        if x == 'en' or x.startswith('en-'):\n",
    "            description_set.add(pydash.get(descriptions[x], 'value'))\n",
    "\n",
    "    # Aliases and labels:\n",
    "    aliases = pydash.get(record, 'aliases')\n",
    "    labels = pydash.get(record, 'labels')\n",
    "    alias_dict = dict()\n",
    "    for x in aliases:\n",
    "        if x in languages or x.startswith('en-'):\n",
    "            for y in aliases[x]:\n",
    "                if \"value\" in y:\n",
    "                    if not y[\"value\"].isupper() and not y[\"value\"].islower() and any(x.isalpha() for x in y[\"value\"]):\n",
    "                        if x in alias_dict:\n",
    "                            if not y[\"value\"] in alias_dict[x]:\n",
    "                                alias_dict[x].append(y[\"value\"])\n",
    "                        else:\n",
    "                            alias_dict[x] = [y[\"value\"]]\n",
    "    for x in labels:\n",
    "        if x in languages or x.startswith('en-'):\n",
    "            if \"value\" in labels[x]:\n",
    "                if not labels[x][\"value\"].isupper() and not labels[x][\"value\"].islower() and any(z.isalpha() for z in labels[x][\"value\"]):\n",
    "                    if x in alias_dict:\n",
    "                        if not labels[x][\"value\"] in alias_dict[x]:\n",
    "                            alias_dict[x].append(labels[x][\"value\"])\n",
    "                    else:\n",
    "                        alias_dict[x] = [labels[x][\"value\"]]\n",
    "\n",
    "    # Native label\n",
    "    nativelabel_dict = pydash.get(record, 'claims.P1705')\n",
    "    nativelabel = None\n",
    "    if nativelabel_dict:\n",
    "        nativelabel = [pydash.get(c, 'mainsnak.datavalue.value.text') for c in nativelabel_dict]\n",
    "\n",
    "    # ==========================================\n",
    "    # Geographic and demographic information\n",
    "    # ==========================================\n",
    "\n",
    "    # Population at: dictionary of year-population pairs\n",
    "    population_dump = pydash.get(record, 'claims.P1082')\n",
    "    population_dict = dict()\n",
    "    if population_dump:\n",
    "        for ppl in population_dump:\n",
    "            pop_amount = pydash.get(ppl, 'mainsnak.datavalue.value.amount')\n",
    "            pop_time = pydash.get(ppl, 'qualifiers.P585[0].datavalue.value.time')\n",
    "            pop_time = \"UNKNOWN\" if not pop_time else pop_time\n",
    "            population_dict[pop_time] = pop_amount\n",
    "\n",
    "    # Area of location\n",
    "    dict_area_units = {'Q712226' : 'square kilometre',\n",
    "               'Q2737347': 'square millimetre',\n",
    "               'Q2489298': 'square centimetre',\n",
    "               'Q35852': 'hectare',\n",
    "               'Q185078': 'are',\n",
    "               'Q25343': 'square metre'}\n",
    "\n",
    "    area_loc = pydash.get(record, 'claims.P2046[0].mainsnak.datavalue.value')\n",
    "    area = None\n",
    "    if area_loc:\n",
    "        try:\n",
    "            if area_loc.get(\"unit\"):\n",
    "                area = (area_loc.get(\"amount\"), dict_area_units.get(area_loc.get(\"unit\").split(\"/\")[-1]))\n",
    "        except:\n",
    "            area = None\n",
    "\n",
    "    # ==========================================\n",
    "    # Historical information\n",
    "    # ==========================================\n",
    "\n",
    "    # Historical counties\n",
    "    hcounties_dict = pydash.get(record, 'claims.P7959')\n",
    "    hcounties = []\n",
    "    if hcounties_dict:\n",
    "        hcounties = [pydash.get(hc, 'mainsnak.datavalue.value.id') for hc in hcounties_dict]\n",
    "\n",
    "    # Date of official opening (e.g. https://www.wikidata.org/wiki/Q2011)\n",
    "    date_opening = pydash.get(record, 'claims.P1619[0].mainsnak.datavalue.value.time')\n",
    "\n",
    "    # Date of official closing\n",
    "    date_closing = pydash.get(record, 'claims.P3999[0].mainsnak.datavalue.value.time')\n",
    "\n",
    "    # Inception: date or point in time when the subject came into existence as defined\n",
    "    inception_date = pydash.get(record, 'claims.P571[0].mainsnak.datavalue.value.time')\n",
    "\n",
    "    # Dissolved, abolished or demolished: point in time at which the subject ceased to exist\n",
    "    dissolved_date = pydash.get(record, 'claims.P576[0].mainsnak.datavalue.value.time')\n",
    "\n",
    "    # Follows...: immediately prior item in a series of which the subject is a part: e.g. Vanuatu follows New Hebrides\n",
    "    follows_dict = pydash.get(record, 'claims.P155')\n",
    "    follows = []\n",
    "    if follows_dict:\n",
    "        for f in follows_dict:\n",
    "            follows.append(pydash.get(f, 'mainsnak.datavalue.value.id'))\n",
    "\n",
    "    # Replaces...: item replaced: e.g. New Hebrides is replaced by \n",
    "    replaces_dict = pydash.get(record, 'claims.P1365')\n",
    "    replaces = []\n",
    "    if replaces_dict:\n",
    "        for r in replaces_dict:\n",
    "            replaces.append(pydash.get(r, 'mainsnak.datavalue.value.id'))\n",
    "\n",
    "    # Heritage designation\n",
    "    heritage_designation = pydash.get(record, 'claims.P1435[0].mainsnak.datavalue.value.id')\n",
    "\n",
    "    # ==========================================\n",
    "    # Neighbouring or part-of locations\n",
    "    # ==========================================\n",
    "\n",
    "    # Located in adminitrative territorial entities (Wikidata ID)\n",
    "    adm_regions_dict = pydash.get(record, 'claims.P131')\n",
    "    adm_regions = dict()\n",
    "    if adm_regions_dict:\n",
    "        for r in adm_regions_dict:\n",
    "            regname = pydash.get(r, 'mainsnak.datavalue.value.id')\n",
    "            if regname:\n",
    "                entity_start_time = pydash.get(r, 'qualifiers.P580[0].datavalue.value.time')\n",
    "                entity_end_time = pydash.get(r, 'qualifiers.P582[0].datavalue.value.time')\n",
    "                adm_regions[regname] = (entity_start_time, entity_end_time)\n",
    "\n",
    "    # Country: sovereign state of this item\n",
    "    country_dict = pydash.get(record, 'claims.P17')\n",
    "    countries = dict()\n",
    "    if country_dict:\n",
    "        for r in country_dict:\n",
    "            countryname = pydash.get(r, 'mainsnak.datavalue.value.id')\n",
    "            if countryname:\n",
    "                entity_start_time = pydash.get(r, 'qualifiers.P580[0].datavalue.value.time')\n",
    "                entity_end_time = pydash.get(r, 'qualifiers.P582[0].datavalue.value.time')\n",
    "                countries[countryname] = (entity_start_time, entity_end_time)\n",
    "\n",
    "    # Continents (Wikidata ID)\n",
    "    continent_dict = pydash.get(record, 'claims.P30')\n",
    "    continents = None\n",
    "    if continent_dict:\n",
    "        continents = [pydash.get(r, 'mainsnak.datavalue.value.id') for r in continent_dict]\n",
    "\n",
    "    # Location is capital of\n",
    "    capital_of_dict = pydash.get(record, 'claims.P1376')\n",
    "    capital_of = None\n",
    "    if capital_of_dict:\n",
    "        capital_of = [pydash.get(r, 'mainsnak.datavalue.value.id') for r in capital_of_dict]\n",
    "\n",
    "    # Shares border with:\n",
    "    shares_border_dict = pydash.get(record, 'claims.P47')\n",
    "    borders = []\n",
    "    if shares_border_dict:\n",
    "        borders = [pydash.get(t, 'mainsnak.datavalue.value.id') for t in shares_border_dict]\n",
    "\n",
    "    # Nearby waterbodies (Wikidata ID)\n",
    "    near_water_dict = pydash.get(record, 'claims.P206')\n",
    "    near_water = None\n",
    "    if near_water_dict:\n",
    "        near_water = [pydash.get(r, 'mainsnak.datavalue.value.id') for r in near_water_dict]\n",
    "\n",
    "    # Nearby waterbodies (Wikidata ID)\n",
    "    part_of_dict = pydash.get(record, 'claims.P361')\n",
    "    part_of = None\n",
    "    if part_of_dict:\n",
    "        part_of = [pydash.get(r, 'mainsnak.datavalue.value.id') for r in part_of_dict] \n",
    "    \n",
    "\n",
    "    # ==========================================\n",
    "    # Coordinates\n",
    "    # ==========================================\n",
    "\n",
    "    # Latitude and longitude:\n",
    "    latitude = pydash.get(record, 'claims.P625[0].mainsnak.datavalue.value.latitude')\n",
    "    longitude = pydash.get(record, 'claims.P625[0].mainsnak.datavalue.value.longitude')\n",
    "    if latitude and longitude:\n",
    "        latitude = round(latitude, 6)\n",
    "        longitude = round(longitude, 6)\n",
    "\n",
    "    # ==========================================\n",
    "    # External data resources IDs\n",
    "    # ==========================================\n",
    "\n",
    "    # English Wikipedia title:\n",
    "    wikititle = pydash.get(record, 'sitelinks.enwiki.title')\n",
    "\n",
    "    # Geonames ID\n",
    "    geonamesID_dict = pydash.get(record, 'claims.P1566')\n",
    "    geonamesIDs = None\n",
    "    if geonamesID_dict:\n",
    "        geonamesIDs = [pydash.get(gn, 'mainsnak.datavalue.value') for gn in geonamesID_dict]\n",
    "\n",
    "    # TOID: TOpographic IDentifier assigned by the Ordnance Survey to identify a feature in Great Britain\n",
    "    toID_dict = pydash.get(record, 'claims.P3120')\n",
    "    toIDs = None\n",
    "    if toID_dict:\n",
    "        toIDs = [pydash.get(t, 'mainsnak.datavalue.value') for t in toID_dict]\n",
    "\n",
    "    # British History Online VCH ID: identifier of a place, in the British History Online digitisation of the Victoria County History\n",
    "    vchID_dict = pydash.get(record, 'claims.P3628')\n",
    "    vchIDs = None\n",
    "    if vchID_dict:\n",
    "        vchIDs = [pydash.get(t, 'mainsnak.datavalue.value') for t in vchID_dict]\n",
    "\n",
    "    # Vision of Britain place ID: identifier of a place\n",
    "    vob_placeID_dict = pydash.get(record, 'claims.P3616')\n",
    "    vob_placeIDs = None\n",
    "    if vob_placeID_dict:\n",
    "        vob_placeIDs = [pydash.get(vobid, 'mainsnak.datavalue.value') for vobid in vob_placeID_dict]\n",
    "\n",
    "    # Vision of Britain unit ID: identifier of an administrative unit\n",
    "    vob_unitID_dict = pydash.get(record, 'claims.P3615')\n",
    "    vob_unitIDs = None\n",
    "    if vob_unitID_dict:\n",
    "        vob_unitIDs = dict()\n",
    "        for vobid in vob_unitID_dict:\n",
    "            unit_id = pydash.get(vobid, 'mainsnak.datavalue.value')\n",
    "            parish_name = pydash.get(vobid, 'qualifiers.P1810[0].datavalue.value')\n",
    "            vob_unitIDs[unit_id] = parish_name\n",
    "\n",
    "    # Identifier for a place in the Historical Gazetteer of England's Place Names website\n",
    "    epns_dict = pydash.get(record, 'claims.P3627')\n",
    "    epns = None\n",
    "    if epns_dict:\n",
    "        epns = [pydash.get(p, 'mainsnak.datavalue.value') for p in epns_dict]\n",
    "\n",
    "    # Identifier in the Getty Thesaurus of Geographic Names\n",
    "    getty_dict = pydash.get(record, 'claims.P1667')\n",
    "    getty = None\n",
    "    if getty_dict:\n",
    "        getty = [pydash.get(p, 'mainsnak.datavalue.value') for p in getty_dict]\n",
    "\n",
    "    # OS grid reference (Wikidata ID)\n",
    "    os_grid_ref = pydash.get(record, 'claims.P613[0].mainsnak.datavalue.value')\n",
    "\n",
    "    # ==========================================\n",
    "    # Street-related properties\n",
    "    # ==========================================\n",
    "\n",
    "    # Street connects with\n",
    "    connectswith_dict = pydash.get(record, 'claims.P2789')\n",
    "    connectswith = None\n",
    "    if connectswith_dict:\n",
    "        connectswith = [pydash.get(c, 'mainsnak.datavalue.value.id') for c in connectswith_dict]\n",
    "\n",
    "    # Street address\n",
    "    street_address = pydash.get(record, 'claims.P6375[0].mainsnak.datavalue.value.text')\n",
    "\n",
    "    # Located on street\n",
    "    street_located = pydash.get(record, 'claims.P669[0].mainsnak.datavalue.value.id')\n",
    "\n",
    "    # Postal code\n",
    "    postal_code_dict = pydash.get(record, 'claims.P281')\n",
    "    postal_code = None\n",
    "    if postal_code_dict:\n",
    "        postal_code = [pydash.get(c, 'mainsnak.datavalue.value') for c in postal_code_dict]\n",
    "\n",
    "    # ==========================================\n",
    "    # Rail-related properties\n",
    "    # ==========================================\n",
    "\n",
    "    # Adjacent stations\n",
    "    adjacent_stations = None\n",
    "    adj_st_dump = pydash.get(record, 'claims.P197')\n",
    "    if adj_st_dump:\n",
    "        adjacent_stations = [pydash.get(adj_st, 'mainsnak.datavalue.value.id') for adj_st in adj_st_dump]\n",
    "\n",
    "    # UK railway station code\n",
    "    ukrailcode_dict = pydash.get(record, 'claims.P4755')\n",
    "    ukrailcode = None\n",
    "    if ukrailcode_dict:\n",
    "        ukrailcode = [pydash.get(ukrid, 'mainsnak.datavalue.value') for ukrid in ukrailcode_dict]\n",
    "\n",
    "    # Connecting lines\n",
    "    connectline_dict = pydash.get(record, 'claims.P81')\n",
    "    connectline = None\n",
    "    if connectline_dict:\n",
    "        connectline = [pydash.get(conline, 'mainsnak.datavalue.value.id') for conline in connectline_dict]\n",
    "\n",
    "    # Owned by\n",
    "    ownedby_dict = pydash.get(record, 'claims.P127')\n",
    "    ownedby = None\n",
    "    if ownedby_dict:\n",
    "        ownedby = [pydash.get(conline, 'mainsnak.datavalue.value.id') for conline in ownedby_dict]\n",
    "\n",
    "    # Connecting service\n",
    "    connectservice_dict = pydash.get(record, 'claims.P1192')\n",
    "    connectservice = None\n",
    "    if connectservice_dict:\n",
    "        connectservice = [pydash.get(conline, 'mainsnak.datavalue.value.id') for conline in connectservice_dict]\n",
    "\n",
    "    # ==========================================\n",
    "    # Store records in a dictionary\n",
    "    # ==========================================\n",
    "    df_record = {'wikidata_id': wikidata_id, 'english_label': english_label,\n",
    "                 'instance_of': instance_of, 'description_set': description_set,\n",
    "                 'alias_dict': alias_dict, 'nativelabel': nativelabel,\n",
    "                 'population_dict': population_dict, 'area': area,\n",
    "                 'hcounties': hcounties, 'date_opening': date_opening,\n",
    "                 'date_closing': date_closing, 'inception_date': inception_date,\n",
    "                 'dissolved_date': dissolved_date, 'follows': follows,\n",
    "                 'replaces': replaces, 'adm_regions': adm_regions,\n",
    "                 'countries': countries, 'continents': continents,\n",
    "                 'capital_of': capital_of, 'borders': borders, 'near_water': near_water,\n",
    "                 'latitude': latitude, 'longitude': longitude, 'wikititle': wikititle,\n",
    "                 'geonamesIDs': geonamesIDs, 'toIDs': toIDs, 'vchIDs': vchIDs,\n",
    "                 'vob_placeIDs': vob_placeIDs, 'vob_unitIDs': vob_unitIDs,\n",
    "                 'epns': epns, 'os_grid_ref': os_grid_ref, 'connectswith': connectswith,\n",
    "                 'street_address': street_address, 'adjacent_stations': adjacent_stations,\n",
    "                 'ukrailcode': ukrailcode, 'connectline': connectline,\n",
    "                 'heritage_designation': heritage_designation, 'getty': getty,\n",
    "                 'street_located': street_located, 'postal_code': postal_code,\n",
    "                 'ownedby': ownedby, 'connectservice': connectservice\n",
    "                }\n",
    "    return df_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse just _one_ Wikidata record (from client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the wikidata ID of the entity you would like to\n",
    "# parse (e.g. \"Q609161\": https://www.wikidata.org/wiki/Q609161):\n",
    "wikidata_entity = \"Q3397374\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the source using a Wikidata client:\n",
    "from wikidata.client import Client\n",
    "client = Client()\n",
    "entity = client.get(wikidata_entity, load=True)\n",
    "record = entity.__dict__['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wikidata_id': 'Q3397374',\n",
       " 'english_label': 'Newport railway station',\n",
       " 'instance_of': ['Q55488'],\n",
       " 'description_set': {'Station in Newport, Wales, United Kingdom',\n",
       "  'railway station in the United Kingdom'},\n",
       " 'alias_dict': {'en-gb': ['Newport High Street'],\n",
       "  'en': ['Newport High Street', 'Newport railway station'],\n",
       "  'cy': ['Gorsaf reilffordd Casnewydd']},\n",
       " 'nativelabel': None,\n",
       " 'population_dict': {},\n",
       " 'area': None,\n",
       " 'hcounties': ['Q1245075'],\n",
       " 'date_opening': '+1850-00-00T00:00:00Z',\n",
       " 'date_closing': None,\n",
       " 'inception_date': None,\n",
       " 'dissolved_date': None,\n",
       " 'follows': [],\n",
       " 'replaces': [],\n",
       " 'adm_regions': {'Q11294004': (None, None)},\n",
       " 'countries': {'Q145': (None, None)},\n",
       " 'continents': None,\n",
       " 'capital_of': None,\n",
       " 'borders': [],\n",
       " 'near_water': None,\n",
       " 'latitude': 51.5896,\n",
       " 'longitude': -2.999,\n",
       " 'wikititle': 'Newport railway station',\n",
       " 'geonamesIDs': ['6953227'],\n",
       " 'toIDs': ['4000000073307859'],\n",
       " 'vchIDs': None,\n",
       " 'vob_placeIDs': None,\n",
       " 'vob_unitIDs': None,\n",
       " 'epns': None,\n",
       " 'os_grid_ref': 'ST309883',\n",
       " 'connectswith': None,\n",
       " 'street_address': None,\n",
       " 'adjacent_stations': ['Q2292817', 'Q2496470', 'Q2210072'],\n",
       " 'ukrailcode': ['NWP'],\n",
       " 'connectline': ['Q1463614',\n",
       "  'Q14992650',\n",
       "  'Q2631964',\n",
       "  'Q1581668',\n",
       "  'Q4969129',\n",
       "  'Q7984143'],\n",
       " 'heritage_designation': None,\n",
       " 'getty': None,\n",
       " 'street_located': None,\n",
       " 'postal_code': None,\n",
       " 'ownedby': ['Q1501071'],\n",
       " 'connectservice': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse the record:\n",
    "parse_record(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the full contents of the wikidata entry:\n",
    "print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37deezy)",
   "language": "python",
   "name": "py37deezy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
